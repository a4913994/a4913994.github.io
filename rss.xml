<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title></title>
        <link>undefined</link>
        <description>undefined</description>
        <lastBuildDate>Thu, 30 Nov 2023 13:56:31 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>Joplin Pages Publisher</generator>
        <item>
            <title><![CDATA[Linux 6.6使用EEVDF调度器替代CFS调度器]]></title>
            <guid>d6ab34cfebe04eeb8f8eb46cc3e38c6d</guid>
            <pubDate>Thu, 30 Nov 2023 13:44:49 GMT</pubDate>
            <content:encoded><![CDATA[<blockquote>
<p>前言：2023年10月30日发布的 Linux 6.6使用“最早合格虚拟期限优先”（Earliest Eligible Virtual Deadline First, EEVDF）替代完全公平调度算法（<a title="https://kernelnewbies.org/Linux_6.6#New_task_scheduler:_EEVDF%EF%BC%89%E3%80%82%E6%9C%AC%E6%96%87%E6%98%AF%E8%8B%B1%E6%96%87%E6%96%87%E6%A1%A3%E2%80%9CAn" href="https://kernelnewbies.org/Linux_6.6#New_task_scheduler:_EEVDF%EF%BC%89%E3%80%82%E6%9C%AC%E6%96%87%E6%98%AF%E8%8B%B1%E6%96%87%E6%96%87%E6%A1%A3%E2%80%9CAn">https://kernelnewbies.org/Linux_6.6#New_task_scheduler:_EEVDF）。本文是英文文档“An</a> EEVDF CPU scheduler for Linux”（<a title="https://lwn.net/Articles/925371/%EF%BC%89%E7%9A%84%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91%E3%80%82" href="https://lwn.net/Articles/925371/%EF%BC%89%E7%9A%84%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91%E3%80%82">https://lwn.net/Articles/925371/）的中文翻译。</a></p>
</blockquote>
<p>内核的完全公平调度器（Completely Fair Scheduler, CFS）负责管理大多数Linux系统上运行的大多数进程的CPU时间分配。CFS在2007年的2.6.23版本中合并，并且从那以后经过多次持续调整，相当好地处理了这项工作。然而CFS不完美，在某些情况下它处理得不如应该的那样好。彼得·齐尔斯特拉（Peter Zijlstra）发布的EEVDF调度器提供了改进CFS的可能性，同时降低了对通常脆弱的启发式的依赖。</p>
<h2 id="cfs和调度约束">CFS和调度约束</h2>
<p>从其名称中可以理解，CFS的关键设计目标之一是公平性，即确保系统中的每个进程都能获得公平的CPU时间份额。这个目标是这样实现的：跟踪每个进程接收的时间，运行那些获得的CPU时间比其他进程少的进程，每个进程的运行时间按其“nice”优先级缩放。换句话说，CFS的核心是一个加权公平排队调度器。</p>
<p>事实证明，公平性足以解决许多CPU调度问题。然而，除了CPU时间的公平分配之外，调度器还受到许多限制。例如，它应该最大限度地提高系统内存缓存的收益，这需要最大限度地减少进程在CPU之间的迁移。但与此同时，如果有工作要做，它应该让所有CPU保持忙碌。电源管理也是一个复杂的问题，有时系统吞吐量的最佳决策必须让位于保护电池寿命。混合系统（并非所有CPU是相同的）增加了更多的复杂性。等等。</p>
<p>一个需要改进的地方是处理延迟要求。一些进程可能不需要大量的CPU时间，但当它们确实需要时间时，它们需要快速获得。其他进程可能需要更多的CPU时间，但如果需要，它们可以等待。CFS没有给进程提供表达延迟要求的方法；nice值（优先级）可用于给进程更多的CPU时间，但这不是一回事。实时调度类可用于对延迟敏感的工作，但在实时类中运行是一种特权操作，实时进程会严重影响系统其余部分的运行。</p>
<p>目前缺乏的是一种方法，可以确保某些进程能够快速访问CPU，而不必让这些进程获得超过其公平份额的CPU时间。latency-nice补丁已经流传了一段时间，作为解决这个问题的尝试；它们允许具有严格延迟要求的CFS进程在想要运行时跳过CPU队列。这些补丁似乎有效，但齐尔斯特拉认为可能有一个更好的方法来解决这个问题。</p>
<h2 id="引入eevdf">引入EEVDF</h2>
<p>“最早合格虚拟期限优先”（Earliest Eligible Virtual Deadline First, EEVDF）调度算法并不新鲜；扬·斯托伊卡（Ion Stoica）和侯赛因·阿卜杜勒-瓦哈卜（Hussein Abdel-Wahab）在1995年的这篇论文“<a title="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=805acf7726282721504c8f00575d91ebfd750564" href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=805acf7726282721504c8f00575d91ebfd750564">Earliest Eligible Virtual Deadline First : A Flexible and Accurate Mechanism for Proportional Share Resource Allocation</a>”中对此进行了描述。它的名字表明它类似于内核的限期调度器使用的最早期限优先算法，但是和这个调度器不同，EEVDF不是实时调度器，所以它的工作方式不同。理解EEVDF需要掌握一些相对简单的概念。</p>
<p>与CFS一样，EEVDF试图在争夺CPU时间的进程之间公平地分配可用的CPU时间。例如，如果有5个进程试图在单个CPU上运行，那么每个进程应获得20%的可用时间。给定进程的nice值可用于调整其公平时间的计算；nice值较低（因此优先级较高）的进程有权以牺牲nice值较高的进程为代价获得更多的CPU时间。就此而言，这里并没有什么新东西。</p>
<p>想象一下，在一秒钟的时间段内，在我们的5进程场景中，每个进程都应该获得 200毫秒的CPU时间。由于多种原因，事情永远不会完全如此；一些进程会获得过多的时间，而另一些进程则会被缩短。对于每个进程，EEVDF计算进程应该获得的时间和实际获得的时间之间的差值；该差值称为“滞后”（lag）。具有正滞后值的进程没有得到公平的份额，应比具有负滞后值的进程更快地被调度。</p>
<p>事实上，如果一个进程计算出来的滞后大于或等于零，那么它就被认为是“合格的”；任何具有负滞后的进程都没有资格运行。对于任何不合格的进程，将来总会有一个时间点，它有权获得的时间赶上它实际获得的时间，并且它将再次变得合格；这个时间点被认为是“合格时间”。</p>
<p>因此，计算滞后是EEVDF调度器的关键部分，补丁集的大部分内容都致力于正确地找到这个值。即使在没有完整的EEVDF算法的情况下，进程的滞后也可以用来将其公平地放置在运行队列中；具有较高滞后的进程应该首先运行，以试图平衡整个系统的滞后值。</p>
<p>另一个起作用的因素是“虚拟期限”，即进程应该收到其应得CPU时间的最早时间。这个期限是通过将进程的分配时间片加到其合格时间来计算的。一个具有10毫秒时间片的进程，合格时间为未来的20毫秒，那么虚拟期限为未来的30毫秒。</p>
<p>EEVDF的核心，正如其名称所示，是它将会优先运行具有最早的虚拟期限的进程。因此调度选择是由公平性（用于计算合格时间的滞后值）和每个进程目前应得的时间量共同驱动的。</p>
<h2 id="解决延迟问题">解决延迟问题</h2>
<p>有了这个框架，对延迟敏感的进程的更快访问就自然实现。当调度器计算每个进程的时间片时，它会考虑该进程分配的latency-nice值；latency-nice值较低（因此延迟要求较严格）的进程将获得较短的时间片。对延迟相对不敏感的进程将获得较长的时间片。请注意，给任何两个具有相同的nice值的进程的CPU时间量将是相同的，但低延迟进程将获得更多的短时间片。</p>
<p>请记住，虚拟期限是通过将时间片加到合格时间来计算的。这将导致时间片较短的进程具有较近的虚拟期限，因此将先执行。对延迟敏感的进程通常不需要大量的CPU时间，能够快速响应事件，而没有延迟要求的进程将获得更长的时间片，这有助于提高吞吐量。不需要复杂的调度器启发式算法来获得这个结果。</p>
<p>然而学术论文和在Linux内核中表现良好的实现之间还有很大的距离。齐尔斯特拉才刚开始在他的EEVDF调度器上运行基准测试；他最初的结论是“有很多成功和失败，但没有表明完全失败”。他说一些结果似乎表明EEVDF调度比CFS更一致，并且有很多延迟优势。</p>
<p>虽然这显然是一个合理的起点，但齐尔斯特拉承认，仍有许多工作要做。但是，他说，“如果我们能做到这一点，我们就可以删除一大堆令人讨厌的启发式代码”，用定义更好的策略取代它。这不是一个小的变化，他补充说：“它完全重新设计了基础调度器、放置、抢占、选择——一切。它们唯一的共同点是它们都是基于虚拟时间的调度器。”</p>
<p>毫无疑问，这样的根本性改变不太可能被仓促地纳入内核。令人欣慰的是，目前的补丁将EEVDF作为CFS的替代方案，这将使更广泛的测试成为可能，而无需实际替换当前的调度器。CPU调度器必须在内核支持的各种系统上为几乎所有可想象的工作负载做正确的事情；这为即使是很小的变化也可能导致不受欢迎的退化留下了很大的空间——而这不是。因此在考虑用EEVDF替换CFS之前必须进行大量的测试；对于何时可能发生，没有虚拟的或其他的期限。</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Linux下的bftrace]]></title>
            <guid>d1622a9f31bd4039a4ddfa26a7c0ec39</guid>
            <pubDate>Sat, 22 Apr 2023 12:36:17 GMT</pubDate>
            <content:encoded><![CDATA[<p>Model: GPT-4</p>
<blockquote>
<p>1 / 1</p>
<p>帮我介绍下Linux下的ftrace</p>
</blockquote>
<p>1 / 1</p>
<p>ftrace（Function Tracer）是 Linux 内核中的一个性能分析和调试工具，它可以帮助开发者在内核运行时实时追踪、监控和分析内核函数的执行。ftrace 的主要功能是提供对内核函数调用的低开销跟踪，以便找到性能瓶颈、故障排查以及优化内核性能。</p>
<p>ftrace 工具的核心是一个名为 tracefs 的文件系统，位于 /sys/kernel/debug/tracing/ 目录下。通过操作这个目录下的文件，可以控制 ftrace 的各种功能。</p>
<p>ftrace 支持多种跟踪事件和插件，如函数图插件 (function_graph)、调度跟踪插件 (sched_switch) 等。以下是一些 ftrace 主要功能：</p>
<ol>
<li>函数跟踪：追踪和记录内核函数的调用和返回，便于查看函数执行顺序和嵌套关系。</li>
<li>函数图：展示函数调用的层次结构，以便更好地理解函数间的关系。</li>
<li>事件跟踪：记录内核中特定事件的发生，例如内存管理、调度、文件系统等。</li>
<li>动态事件：根据用户自定义的事件，实时收集和过滤相关数据。</li>
<li>实时调试：在内核运行时启用或禁用跟踪功能，无需重新编译或重启内核。</li>
</ol>
<p>ftrace 的使用方法举例：</p>
<ol>
<li>启用函数跟踪：</li>
</ol>
<p>shell</p>
<div><pre class="hljs"><code><span class="hljs-meta"># </span><span class="language-bash"><span class="hljs-built_in">echo</span> <span class="hljs-keyword">function</span> &gt; /sys/kernel/debug/tracing/current_tracer</span></code></pre></div>
<ol start="2">
<li>记录跟踪数据：</li>
</ol>
<p>shell</p>
<div><pre class="hljs"><code><span class="hljs-meta"># </span><span class="language-bash">cat /sys/kernel/debug/tracing/trace</span></code></pre></div>
<ol start="3">
<li>关闭函数跟踪：</li>
</ol>
<p>shell</p>
<div><pre class="hljs"><code><span class="hljs-meta"># </span><span class="language-bash"><span class="hljs-built_in">echo</span> nop &gt; /sys/kernel/debug/tracing/current_tracer</span></code></pre></div>
<p>总之，ftrace 是一个功能强大的 Linux 内核性能分析和调试工具，能够帮助开发者深入了解内核的运行机制、定位问题和优化性能。</p>
<blockquote>
<p>1 / 1</p>
<p>ftrace可以同时追踪多个函数吗</p>
</blockquote>
<p>1 / 1</p>
<p>是的，ftrace 可以同时追踪多个函数。要实现这个功能，您可以使用 set_ftrace_filter 文件来指定您想要追踪的函数列表。这个文件位于 /sys/kernel/debug/tracing/ 目录下。</p>
<p>以下是同时追踪多个函数的操作步骤：</p>
<ol>
<li>启用函数跟踪：</li>
</ol>
<p>shell</p>
<div><pre class="hljs"><code><span class="hljs-meta"># </span><span class="language-bash"><span class="hljs-built_in">echo</span> <span class="hljs-keyword">function</span> &gt; /sys/kernel/debug/tracing/current_tracer</span></code></pre></div>
<ol start="2">
<li>清空默认的函数过滤器（如果有的话）：</li>
</ol>
<p>shell</p>
<div><pre class="hljs"><code><span class="hljs-meta"># </span><span class="language-bash"><span class="hljs-built_in">echo</span> &gt; /sys/kernel/debug/tracing/set_ftrace_filter</span></code></pre></div>
<ol start="3">
<li>添加要追踪的函数，例如同时追踪 do_fork 和 vfs_read 函数：</li>
</ol>
<p>shell</p>
<div><pre class="hljs"><code><span class="hljs-meta"># </span><span class="language-bash"><span class="hljs-built_in">echo</span> do_fork &gt; /sys/kernel/debug/tracing/set_ftrace_filter</span>
<span class="hljs-meta"># </span><span class="language-bash"><span class="hljs-built_in">echo</span> vfs_read &gt;&gt; /sys/kernel/debug/tracing/set_ftrace_filter</span></code></pre></div>
<p>注意，使用 "&gt;&gt;" 而不是 "&gt;" 是因为我们需要将新函数添加到列表中，而不是覆盖已有的函数列表。</p>
<ol start="4">
<li>确认已添加的函数：</li>
</ol>
<p>shell</p>
<div><pre class="hljs"><code><span class="hljs-meta"># </span><span class="language-bash">cat /sys/kernel/debug/tracing/set_ftrace_filter</span></code></pre></div>
<ol start="5">
<li>查看跟踪结果：</li>
</ol>
<p>shell</p>
<div><pre class="hljs"><code><span class="hljs-meta"># </span><span class="language-bash">cat /sys/kernel/debug/tracing/trace</span></code></pre></div>
<ol start="6">
<li>结束跟踪时，关闭函数跟踪：</li>
</ol>
<p>shell</p>
<div><pre class="hljs"><code><span class="hljs-meta"># </span><span class="language-bash"><span class="hljs-built_in">echo</span> nop &gt; /sys/kernel/debug/tracing/current_tracer</span></code></pre></div>
<p>通过这些步骤，您可以实现对多个内核函数的同时追踪。</p>
<blockquote>
<p>1 / 1</p>
<p>追踪结果只能通过/sys/kernel/debug/tracing/trace来查看吗</p>
</blockquote>
<p>1 / 1</p>
<p>通过查看 <code>/sys/kernel/debug/tracing/trace</code> 文件是 ftrace 的标准方式来查看追踪结果，但您也可以使用其他方法。以下是几种在 Linux 系统中查看 ftrace 追踪结果的方法：</p>
<ol>
<li>使用 <code>cat</code> 或 <code>less</code> 命令直接查看 trace 文件：</li>
</ol>
<p>shell</p>
<div><pre class="hljs"><code><span class="hljs-meta"># </span><span class="language-bash">cat /sys/kernel/debug/tracing/trace</span></code></pre></div>
<p>或</p>
<p>shell</p>
<div><pre class="hljs"><code><span class="hljs-meta"># </span><span class="language-bash">less /sys/kernel/debug/tracing/trace</span></code></pre></div>
<ol start="2">
<li>使用 <code>trace-cmd</code> 工具来记录和查看 ftrace 数据。<code>trace-cmd</code> 是一个用于操作 ftrace 的用户空间工具，可以方便地记录、查看和处理 ftrace 数据。首先，您需要安装 <code>trace-cmd</code> 工具。然后，您可以使用如下命令：</li>
</ol>
<p>arduino</p>
<div><pre class="hljs"><code><span class="hljs-meta"># trace-cmd record -e sched_switch -e irq_handler_entry -o trace.dat</span></code></pre></div>
<p>上述命令会将指定事件的追踪结果记录到 trace.dat 文件中。接下来，使用下面的命令查看追踪结果：</p>
<p>arduino</p>
<div><pre class="hljs"><code><span class="hljs-meta"># trace-cmd report trace.dat</span></code></pre></div>
<ol start="3">
<li>使用 <code>perf</code> 工具来记录和查看 ftrace 数据。<code>perf</code> 是一个通用的 Linux 性能分析工具，它也支持 ftrace 事件。例如，要使用 <code>perf</code> 记录调度事件，您可以执行以下命令：</li>
</ol>
<p>shell</p>
<div><pre class="hljs"><code><span class="hljs-meta"># </span><span class="language-bash">perf record -e <span class="hljs-built_in">sched</span>:sched_switch -a</span></code></pre></div>
<p>然后，使用 <code>perf report</code> 命令查看追踪结果：</p>
<p>arduino</p>
<div><pre class="hljs"><code><span class="hljs-meta"># perf report</span></code></pre></div>
<p>总之，虽然查看 <code>/sys/kernel/debug/tracing/trace</code> 文件是查看 ftrace 追踪结果的直接方法，但您还可以选择其他方法，如使用 <code>trace-cmd</code> 或 <code>perf</code> 工具来查看和分析追踪数据。这些工具提供了更丰富的功能和更方便的操作方式。</p>
<blockquote>
<p>1 / 1</p>
<p>perf可以采集所有的追踪事件吗</p>
</blockquote>
<p>1 / 1</p>
<p><code>perf</code> 是一个通用的 Linux 性能分析工具，它可以采集多种内核和用户空间的追踪事件。<code>perf</code> 支持采集诸如 CPU 性能计数器事件、内核跟踪点 (tracepoints) 事件、硬件和软件事件等。然而，<code>perf</code> 并不直接支持所有 ftrace 事件和功能。尽管 <code>perf</code> 可以采集许多 ftrace 支持的事件，但它们的底层实现和工作原理有所不同。</p>
<p><code>perf</code> 支持以下几类事件：</p>
<ol>
<li>硬件事件：如 CPU 缓存命中、分支预测错误等。</li>
<li>软件事件：如 CPU 时钟周期、任务切换、页面错误等。</li>
<li>内核跟踪点 (tracepoints)：这些事件是在内核源代码中特定位置预定义的跟踪点，可以用于监控内核子系统（如调度、内存管理、文件系统等）的运行情况。</li>
<li>动态探针 (kprobes) 和用户空间动态探针 (uprobes)：允许在内核和用户空间代码中的任意位置插入探针，以监控特定函数或指令的执行情况。</li>
</ol>
<p>尽管 <code>perf</code> 和 ftrace 在事件采集和性能分析方面有很多相似之处，但它们的设计目标和关注点略有不同。ftrace 更侧重于内核函数调用的跟踪和分析，而 <code>perf</code> 更侧重于全面的性能分析，包括硬件和软件事件以及用户空间程序的性能分析。</p>
<p>总之，<code>perf</code> 可以采集许多追踪事件，包括一些 ftrace 支持的事件，但它们并不是完全等价的。在实际使用中，您可以根据需要选择合适的工具来完成性能分析和调试任务。</p>
<blockquote>
<p>1 / 1</p>
<p>我可以写个Golang程序将这两者结合在一起吗</p>
</blockquote>
<p>1 / 1</p>
<p>当然可以。您可以编写一个 Golang 程序，利用 Golang 的系统调用和文件操作功能，结合 <code>perf</code> 和 ftrace 工具来实现混合的性能分析和调试功能。以下是一个简单的示例，展示了如何在 Golang 程序中使用 <code>perf</code> 和 ftrace 功能：</p>
<ol>
<li>首先，安装必要的依赖包。例如，您可以使用 <code>github.com/iovisor/gobpf</code> 包，它提供了一个 Golang 接口来操作 <code>perf</code> 事件和 eBPF 程序。要安装该包，请运行以下命令：</li>
</ol>
<p>arduino</p>
<div><pre class="hljs"><code>go get -u github.com/iovisor/gobpf</code></pre></div>
<ol start="2">
<li>编写 Golang 程序。以下是一个简单的示例，它同时使用 ftrace 和 <code>perf</code> 工具来跟踪内核函数调用和硬件事件：</li>
</ol>
<p>go</p>
<div><pre class="hljs"><code><span class="hljs-keyword">package</span> main

<span class="hljs-keyword">import</span> (
    <span class="hljs-string">"fmt"</span>
    <span class="hljs-string">"io/ioutil"</span>
    <span class="hljs-string">"os"</span>
    <span class="hljs-string">"os/exec"</span>
    <span class="hljs-string">"time"</span>

    bpf <span class="hljs-string">"github.com/iovisor/gobpf/pkg/bpflib"</span>
)

<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> {
    <span class="hljs-comment">// 使用 ftrace 跟踪内核函数 'do_fork'</span>
    ftraceFunction := <span class="hljs-string">"do_fork"</span>
    ftraceFile := <span class="hljs-string">"/sys/kernel/debug/tracing/set_ftrace_filter"</span>

    err := ioutil.WriteFile(ftraceFile, []<span class="hljs-keyword">byte</span>(ftraceFunction), <span class="hljs-number">0644</span>)
    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        fmt.Println(<span class="hljs-string">"Error writing ftrace filter:"</span>, err)
        <span class="hljs-keyword">return</span>
    }

    err = ioutil.WriteFile(<span class="hljs-string">"/sys/kernel/debug/tracing/current_tracer"</span>, []<span class="hljs-keyword">byte</span>(<span class="hljs-string">"function"</span>), <span class="hljs-number">0644</span>)
    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        fmt.Println(<span class="hljs-string">"Error setting ftrace tracer:"</span>, err)
        <span class="hljs-keyword">return</span>
    }

    <span class="hljs-comment">// 使用 perf 记录 CPU 缓存引用事件</span>
    perfCmd := exec.Command(<span class="hljs-string">"perf"</span>, <span class="hljs-string">"record"</span>, <span class="hljs-string">"-e"</span>, <span class="hljs-string">"cache-references"</span>, <span class="hljs-string">"-a"</span>, <span class="hljs-string">"--"</span>, <span class="hljs-string">"sleep"</span>, <span class="hljs-string">"5"</span>)

    err = perfCmd.Start()
    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        fmt.Println(<span class="hljs-string">"Error starting perf record:"</span>, err)
        <span class="hljs-keyword">return</span>
    }

    <span class="hljs-comment">// 保持 ftrace 和 perf 同时运行 5 秒钟</span>
    time.Sleep(<span class="hljs-number">5</span> * time.Second)

    <span class="hljs-comment">// 关闭 ftrace</span>
    err = ioutil.WriteFile(<span class="hljs-string">"/sys/kernel/debug/tracing/current_tracer"</span>, []<span class="hljs-keyword">byte</span>(<span class="hljs-string">"nop"</span>), <span class="hljs-number">0644</span>)
    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        fmt.Println(<span class="hljs-string">"Error disabling ftrace tracer:"</span>, err)
        <span class="hljs-keyword">return</span>
    }

    <span class="hljs-comment">// 等待 perf 结束</span>
    err = perfCmd.Wait()
    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        fmt.Println(<span class="hljs-string">"Error waiting for perf record:"</span>, err)
        <span class="hljs-keyword">return</span>
    }

    <span class="hljs-comment">// 读取并显示 ftrace 数据</span>
    ftraceData, err := ioutil.ReadFile(<span class="hljs-string">"/sys/kernel/debug/tracing/trace"</span>)
    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        fmt.Println(<span class="hljs-string">"Error reading ftrace data:"</span>, err)
        <span class="hljs-keyword">return</span>
    }
    fmt.Println(<span class="hljs-string">"Ftrace data:\n"</span>, <span class="hljs-keyword">string</span>(ftraceData))

    <span class="hljs-comment">// 显示 perf 数据</span>
    perfReport := exec.Command(<span class="hljs-string">"perf"</span>, <span class="hljs-string">"report"</span>)
    perfReport.Stdout = os.Stdout
    perfReport.Stderr = os.Stderr

    err = perfReport.Run()
    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        fmt.Println(<span class="hljs-string">"Error running perf report:"</span>, err)
        <span class="hljs-keyword">return</span>
    }
}</code></pre></div>
<p>这个示例程序使用 ftrace 跟踪 <code>do_fork</code> 函数的调用，并使用 <code>perf</code> 工具记录 CPU 缓存引用事件</p>
<blockquote>
<p>1 / 1</p>
<p>这里面没有使用gobpf吗</p>
</blockquote>
<p>1 / 1</p>
<p>抱歉，我犯了一个错误。在我给出的示例代码中，没有使用 <code>gobpf</code> 包。<code>gobpf</code> 主要用于与 eBPF（Extended Berkeley Packet Filter）程序交互，而在前面的示例中，我们仅使用了 ftrace 和 perf 工具。</p>
<p>如果您希望在 Golang 程序中使用 <code>gobpf</code>，可以参考以下示例，这个示例将展示如何使用 <code>gobpf</code> 来加载一个简单的 eBPF 程序，并使用 <code>perf</code> 工具来处</p>
<blockquote>
<p>1 / 1</p>
<p>如何让ebpf在linux3.10上运行</p>
</blockquote>
<p>1 / 1</p>
<p>eBPF（Extended Berkeley Packet Filter）在较旧的 Linux 内核版本（如 3.10）上的支持是有限的。eBPF 需要较新的内核版本（4.x 及更高版本）才能完全发挥其强大功能。在 Linux 3.10 内核上，您只能使用有限的经典 BPF 功能，而无法使用 eBPF 提供的许多高级特性。</p>
<p>如果您的应用程序需要 eBPF 功能，并且您受限于使用 Linux 3.10 内核，您可以考虑以下几种方法：</p>
<ol>
<li>升级内核：尽量升级到较</li>
</ol>
<blockquote>
<p>1 / 1</p>
<p>经典BPF如何在Linux3.10上使用</p>
</blockquote>
<p>1 / 1</p>
<p>在 Linux 3.10 内核上，您可以使用经典 BPF (Berkeley Packet Filter) 功能来进行网络数据包过滤和捕获。经典 BPF 主要用于与 <code>tcpdump</code> 和 <code>libpcap</code> 等工具一起使用，它们可以过滤、捕获和分析网络数据包。</p>
<p>以下是在 Linux 3.10 上使用经典 BPF 的一些方法：</p>
<ol>
<li>使用 <code>tcpdump</code> 工具：</li>
</ol>
<p><code>tcpdump</code> 是一个常用的网络数据包捕获和分析工具，它内置了对经典 BPF 的支持。在命令行中，您可以使用 <code>tcpdump</code> 并指定 BPF 过滤器表达式来捕获感兴趣的数据包。例如，要捕获所有目标端口为 80 的 TCP 数据包，您可以执行以下命令：</p>
<p>arduino</p>
<div><pre class="hljs"><code><span class="hljs-meta"># tcpdump -i eth0 <span class="hljs-string">"tcp port 80"</span></span></code></pre></div>
<p>在这里，`-i eth0</p>
<blockquote>
<p>1 / 1</p>
<p>经典BPF只能捕捉网络信息吗</p>
</blockquote>
<p>1 / 1</p>
<p>经典 BPF（Berkeley Packet Filter）最初是为了高效捕获和过滤网络数据包而设计的。因此，它的主要应用场景是网络数据包捕获和分析。然而，经典 BPF 不仅仅局限于网络领域。随着时间的推移，BPF 已经演变成一个更通用的内核虚拟机，可以用于其他类型的内核事件处理。</p>
<p>尽管如此，在 Linux 3.10 内核上，经典 BPF 的功能相对有限，主要用于网络数据包捕获。在这个内核版本上，您无法使用 eBPF（Extended Berkeley Packet Filter）提供的许多高级功能，例如跟踪内核函数、系统调用、用户空间程序等。</p>
<p>总之，在 Linux 3.10 内核上，经典 BPF 的功能主要集中在网络数据包捕获和分析方面。要实现更高级的功能，您需要升级到支持 eBPF 的较新内核版本（4.x 及更高版本）。</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OOM的架构图]]></title>
            <guid>509a44f1ca4e4183b934e644bec7c476</guid>
            <pubDate>Sat, 22 Apr 2023 12:34:53 GMT</pubDate>
            <content:encoded><![CDATA[<p>Model: Default (GPT-3.5)</p>
<blockquote>
<p>1 / 1</p>
<p>我希望你充当 Linux内核专家。我会向您提供有关我的技术问题所需的所有信息，而您的职责是解决我的问题。</p>
</blockquote>
<p>1 / 1</p>
<p>好的，我会尽力帮助您解决关于 Linux 内核的技术问题。请提供关于您所遇到问题的详细信息，包括错误信息、系统环境、您尝试过的解决方案等。这将有助于我更准确地判断问题并提供有效的解决方案。</p>
<blockquote>
<p>1 / 1</p>
<p>用char字符图，画出OOM相关的内存结构</p>
</blockquote>
<p>1 / 1</p>
<p>在 Linux 内核中，内存管理是一个相当复杂的过程，涉及多个数据结构。为了简化问题并更好地理解与 OOM（Out-of-Memory）相关的内存结构，我们将关注以下几个主要组件：</p>
<ol>
<li>mm_struct：这个结构表示一个进程的内存描述符。它包含了与虚拟内存区域（VMAs）有关的信息以及其他内存管理相关的数据。</li>
<li>vm_area_struct：这个结构表示一个虚拟内存区域，它在进程地址空间中定义了一个连续的虚拟内存范围。每个 VMA 都与一个 mm_struct 实例相关联。</li>
<li>page：这个结构表示物理内存中的一个页面。Linux 内核使用 page 结构来跟踪物理内存的使用情况。</li>
<li>mem_cgroup：内存控制组(memcg)是一种用于将内存资源划分给不同任务的机制。mem_cgroup 结构表示一个内存控制组，其中包含与该组相关的内存使用信息。</li>
</ol>
<p>以字符图的形式表示，这些结构可以如下所示：</p>
<p>scss</p>
<div><pre class="hljs"><code>mm_struct
├─── VMAs (vm_area_struct)
│    ├─── Start <span class="hljs-selector-tag">Address</span>
│    ├─── End <span class="hljs-selector-tag">Address</span>
│    ├─── Flags (e.g., Read, Write, Execute)
│    └─── Anon/Private or File/Shared pages
│
├─── Page Tables
│    └─── Mapping of Virtual Addresses to Physical Pages
│
└─── Other memory management related fields
     ├─── rss_stat (Resident Set Size Statistics)
     ├─── mmap_sem (Semaphore for mmap operations)
     └─── ...

page
├─── Physical <span class="hljs-selector-tag">Address</span>
├─── Flags (e.g., Dirty, Accessed)
├─── Reference Counter
└─── LRU (Least Recently Used) List

mem_cgroup
├─── Memory Usage (per cgroup)
├─── Memory Limit (per cgroup)
├─── Soft Limit (per cgroup)
└─── OOM Control (OOM killer settings)</code></pre></div>
<p>请注意，这里仅展示了与 OOM 相关的部分内存结构，Linux 内核的内存管理涉及到更多的数据结构和细节。如果您对这些概念有更多的问题，请随时提问。</p>
<blockquote>
<p>1 / 1</p>
<p>帮我画出为什么会出现OOM的char图</p>
</blockquote>
<p>1 / 1</p>
<p>在 Linux 系统中，当可用内存资源不足以满足当前进程的内存需求时，就会触发 Out-of-Memory (OOM) 状态。OOM 的发生可能由多种原因引起，例如内存泄漏、资源竞争或配置错误。以下是一个简化的字符图，说明了导致 OOM 状态的一些关键因素：</p>
<p>arduino</p>
<div><pre class="hljs"><code>┌────────────────────────────┐
│       System Memory         │
│ ┌────────────────────────┐ │
│ │      Free Memory        │ │
│ └────────────────────────┘ │
│ ┌────────────────────────┐ │
│ │   Memory Cgroup <span class="hljs-number">1</span>       │ │
│ │ ┌─────────────────────┐ │ │
│ │ │  <span class="hljs-built_in">Process</span> A          │ │ │
│ │ │  ┌─────────────────┐│ │ │
│ │ │  │ Allocates more  ││ │ │
│ │ │  │ memory than     ││ │ │
│ │ │  │ available <span class="hljs-keyword">or</span>     ││ │ │
│ │ │  │ reaches limit    ││ │ │
│ │ │  └─────────────────┘│ │ │
│ │ └─────────────────────┘ │ │
│ └────────────────────────┘ │
│ ┌────────────────────────┐ │
│ │   Memory Cgroup <span class="hljs-number">2</span>       │ │
│ │ ┌─────────────────────┐ │ │
│ │ │  <span class="hljs-built_in">Process</span> B          │ │ │
│ │ │  ┌─────────────────┐│ │ │
│ │ │  │ Allocates more  ││ │ │
│ │ │  │ memory than     ││ │ │
│ │ │  │ available <span class="hljs-keyword">or</span>     ││ │ │
│ │ │  │ reaches limit    ││ │ │
│ │ │  └─────────────────┘│ │ │
│ │ └─────────────────────┘ │ │
│ └────────────────────────┘ │
└────────────────────────────┘</code></pre></div>
<p>在这个示例中，系统内存被划分为多个内存控制组（Memory Cgroup）。当进程 A 或进程 B 尝试分配比其内存控制组可用的更多内存时，就会导致 OOM 状态。</p>
<p>OOM Killer 是 Linux 内核中的一个组件，当系统检测到内存不足时，它会根据一系列标准（如进程的内存使用、运行时间、优先级等）来选择并终止一个或多个进程，以释放内存资源。</p>
<p>这个示例仅是为了说明 OOM 的基本概念，实际上可能涉及更多的因素。如果您需要进一步的解释或帮助，请随时提问。</p>
<blockquote>
<p>1 / 1</p>
<p>请将虚拟内存和物理内存的概念也画上去</p>
</blockquote>
<p>1 / 1</p>
<p>虚拟内存和物理内存是内存管理中的两个核心概念。虚拟内存为每个进程提供了一个独立的、连续的地址空间，从而简化了内存管理。物理内存是实际的 RAM 资源，它被划分为大小固定的页面（通常为 4KB）。以下是一个字符图，展示了虚拟内存、物理内存以及它们之间的映射关系：</p>
<p>css</p>
<div><pre class="hljs"><code>┌────────────────────────────────────┐
│       Virtual Memory (Process <span class="hljs-selector-tag">A</span>)    │
│ ┌────────────────────────────────┐ │
│ │   Virtual Memory Area (VMA) <span class="hljs-number">1</span>   │ │
│ │  ┌────────────────────────────┐│ │
│ │  │ Virtual <span class="hljs-selector-tag">Address</span> <span class="hljs-number">1</span>          ││ │
│ │  └────────────────────────────┘│ │
│ └────────────────────────────────┘ │
│ ┌────────────────────────────────┐ │
│ │   Virtual Memory Area (VMA) <span class="hljs-number">2</span>   │ │
│ │  ┌────────────────────────────┐│ │
│ │  │ Virtual <span class="hljs-selector-tag">Address</span> <span class="hljs-number">2</span>          ││ │
│ │  └────────────────────────────┘│ │
│ └────────────────────────────────┘ │
└────────────────────────────────────┘
                     │
                     │
                     ▼
┌────────────────────────────┐
│        Page Tables          │
│ ┌────────────────────────┐ │
│ │  Virtual <span class="hljs-selector-tag">Address</span> <span class="hljs-number">1</span>      │ │
│ │  └───┬─────────────────┘ │
│ │      │                   │
│ │  Virtual <span class="hljs-selector-tag">Address</span> <span class="hljs-number">2</span>      │ │
│ │  └───┬─────────────────┘ │
│ └──────┴──────────────────┘ │
└────────────────────────────┘
                     │
                     │
                     ▼
┌────────────────────────────┐
│       Physical Memory      │
│ ┌────────────────────────┐ │
│ │  Physical Page <span class="hljs-number">1</span>        │ │
│ └────────────────────────┘ │
│ ┌────────────────────────┐ │
│ │  Physical Page <span class="hljs-number">2</span>        │ │
│ └────────────────────────┘ │
└────────────────────────────┘</code></pre></div>
<p>在这个示例中，Process A 的虚拟内存被划分为两个虚拟内存区域（VMA 1 和 VMA 2）。虚拟地址 1 和虚拟地址 2 分别属于这两个区域。Page Tables 负责将虚拟地址映射到物理内存中的实际页面。在这种情况下，虚拟地址 1 和虚拟地址 2 分别映射到物理页面 1 和物理页面 2。</p>
<p>请注意，这个示例仅展示了虚拟内存和物理内存之间映射关系的基本概念。实际上，Linux 内核使用多级页表来进行映射，并涉及到更多的细节。如果您需要进一步的解释</p>
<blockquote>
<p>1 / 1</p>
<p>请将虚拟内存，物理内存和OOM的关系画出来</p>
</blockquote>
<p>1 / 1</p>
<p>以下字符图展示了虚拟内存、物理内存和 OOM 之间的关系。在这个示例中，Process A 尝试分配虚拟内存，该内存将映射到物理内存。当物理内存耗尽时，会触发 OOM 状态。</p>
<p>css</p>
<div><pre class="hljs"><code>┌────────────────────────────────────┐
│       Virtual Memory (Process <span class="hljs-selector-tag">A</span>)    │
│ ┌────────────────────────────────┐ │
│ │   Virtual Memory Area (VMA)    │ │
│ │  ┌────────────────────────────┐│ │
│ │  │ Virtual <span class="hljs-selector-tag">Address</span>            ││ │
│ │  └────────────────────────────┘│ │
│ └────────────────────────────────┘ │
└────────────────────────────────────┘
                     │
                     │
                     ▼
┌────────────────────────────┐
│        Page Tables          │
│ ┌────────────────────────┐ │
│ │  Virtual <span class="hljs-selector-tag">Address</span>        │ │
│ │  └───┬─────────────────┘ │
│ └──────┴──────────────────┘ │
└────────────────────────────┘
                     │
                     │
                     ▼
┌────────────────────────────┐
│       Physical Memory      │
│ ┌────────────────────────┐ │
│ │  Physical Page <span class="hljs-number">1</span>        │ │
│ └────────────────────────┘ │
│ ┌────────────────────────┐ │
│ │  Physical Page <span class="hljs-number">2</span>        │ │
│ └────────────────────────┘ │
│          ...               │
│ ┌────────────────────────┐ │
│ │  Physical Page N        │ │
│ └────────────────────────┘ │
└────────────────────────────┘
                     │
                     │
                     ▼
┌────────────────────────────┐
│           OOM              │
│ ┌────────────────────────┐ │
│ │ Not enough physical     │ │
│ │ memory <span class="hljs-selector-tag">to</span> fulfill       │ │
│ │ virtual memory request  │ │
│ └────────────────────────┘ │
└────────────────────────────┘</code></pre></div>
<p>在这个示例中，Process A 的虚拟内存被划分为一个或多个虚拟内存区域（VMA）。虚拟地址属于这些区域。Page Tables 负责将虚拟地址映射到物理内存中的实际页面。当进程尝试分配虚拟内存时，Linux 内核会分配对应的物理页面。然而，当物理内存耗尽以至于无法分配足够的物理页面时，将触发 OOM 状态。这可能导致 OOM Killer 组件选择并终止一个或多个进程，以释放内存资源。</p>
<p>这个示例仅展示了虚拟内存、物理内存和 OOM 之间关系的基本概念。实际上，Linux 内核使用多级页表来进行映射，并涉及到更多的细节。如果您需要进一步的解释或帮助，请随时提问。</p>
<blockquote>
<p>1 / 1</p>
<p>请将NUMA和OOM的关系画出来</p>
</blockquote>
<p>The previous model used in this conversation is unavailable. We've switched you to the latest default model</p>
<p>1 / 1</p>
<p>NUMA（非统一内存访问）是一种内存架构，其中物理内存被划分为多个 NUMA 节点。每个 NUMA 节点都具有自己的物理内存和 CPU。NUMA 通过减少内存访问延迟来提高系统性能，但也会增加一些管理内存的复杂性。以下是一个字符图，说明了 NUMA 和 OOM 之间的关系：</p>
<p>sql</p>
<div><pre class="hljs"><code>┌──────────────────────────────────────┐
│            NUMA Architecture          │
│ ┌──────────────────────────────────┐ │
│ │           NUMA Node <span class="hljs-number">1</span>            │ │
│ │  ┌────────────────────────────┐  │ │
│ │  │       Physical Memory       │  │ │
│ │  └────────────────────────────┘  │ │
│ │  ┌────────────────────────────┐  │ │
│ │  │        CPU Cores            │  │ │
│ │  └────────────────────────────┘  │ │
│ └──────────────────────────────────┘ │
│ ┌──────────────────────────────────┐ │
│ │           NUMA Node <span class="hljs-number">2</span>            │ │
│ │  ┌────────────────────────────┐  │ │
│ │  │       Physical Memory       │  │ │
│ │  └────────────────────────────┘  │ │
│ │  ┌────────────────────────────┐  │ │
│ │  │        CPU Cores            │  │ │
│ │  └────────────────────────────┘  │ │
│ └──────────────────────────────────┘ │
└──────────────────────────────────────┘
                     │
                     │
                     ▼
┌──────────────────────────────────────┐
│            OOM <span class="hljs-keyword">Condition</span>              │
│ ┌──────────────────────────────────┐ │
│ │     Memory Demand Exceeds         │ │
│ │     Available Memory <span class="hljs-keyword">on</span> a         │ │
│ │     <span class="hljs-keyword">Specific</span> NUMA Node            │ │
│ └──────────────────────────────────┘ │
└──────────────────────────────────────┘</code></pre></div>
<p>在这个示例中，物理内存被划分为两个 NUMA 节点，每个节点具有自己的 CPU 和物理内存。当进程在一个 NUMA 节点上分配大量内存时，可能会导致该节点的物理内存耗尽，从而触发 OOM 状态。由于 NUMA 架构中内存访问具有本地性，因此在内存不足的情况下，如果可以从其他 NUMA 节点中获取内存，则可能会有更好的性能。然而，这也增加了管理内存的复杂性。</p>
<p>请注意，这个示例仅展示了 NUMA 和 OOM 之间关系的基本概念。实际上，NUMA 和 OOM 的关系可能会更加复杂，并涉及到更多的细节。如果您需要进一步的解释或帮助，请随时提问。</p>
<blockquote>
<p>1 / 1</p>
<p>请将NUMA, CGroup和OOM的关系画出来</p>
</blockquote>
<p>1 / 1</p>
<p>在 Linux 系统中，CGroup 是一种机制，用于限制进程对系统资源（如 CPU、内存、磁盘等）的访问。在 NUMA 系统中，CGroup 还可以用于管理进程对不同 NUMA 节点上的内存的访问。以下是一个字符图，说明了 NUMA、CGroup 和 OOM 之间的关系：</p>
<p>arduino</p>
<div><pre class="hljs"><code>┌──────────────────────────────────────┐
│            NUMA Architecture          │
│ ┌──────────────────────────────────┐ │
│ │           NUMA Node <span class="hljs-number">1</span>            │ │
│ │  ┌────────────────────────────┐  │ │
│ │  │       Physical Memory       │  │ │
│ │  └────────────────────────────┘  │ │
│ │  ┌────────────────────────────┐  │ │
│ │  │        CPU Cores            │  │ │
│ │  └────────────────────────────┘  │ │
│ │ ┌─────────────────────────────┐ │ │
│ │ │       Memory Cgroup <span class="hljs-number">1</span>       │ │ │
│ │ │ ┌─────────────────────────┐ │ │ │
│ │ │ │  <span class="hljs-built_in">Process</span> A              │ │ │ │
│ │ │ │  ┌─────────────────────┐│ │ │ │
│ │ │ │  │ Allocates more memory││ │ │ │
│ │ │ │  │ than available <span class="hljs-keyword">or</span>    ││ │ │ │
│ │ │ │  │ reaches limit        ││ │ │ │
│ │ │ │  └─────────────────────┘│ │ │ │
│ │ │ └─────────────────────────┘ │ │ │
│ │ └─────────────────────────────┘ │ │
│ └──────────────────────────────────┘ │
│ ┌──────────────────────────────────┐ │
│ │           NUMA Node <span class="hljs-number">2</span>            │ │
│ │  ┌────────────────────────────┐  │ │
│ │  │       Physical Memory       │  │ │
│ │  └────────────────────────────┘  │ │
│ │  ┌────────────────────────────┐  │ │
│ │  │        CPU Cores            │  │ │
│ │  └────────────────────────────┘  │ │
│ │ ┌─────────────────────────────┐ │ │
│ │ │       Memory Cgroup <span class="hljs-number">2</span>       │ │ │
│ │ │ ┌─────────────────────────┐ │ │ │
│ │ │ │  <span class="hljs-built_in">Process</span> B              │ │ │ │
│ │ │ │  ┌─────────────────────┐│ │ │ │
│ │ │ │  │ Allocates more memory││ │ │ │
│ │ │ │  │ than available <span class="hljs-keyword">or</span>    ││ │ │ │
│ │ │ │  │ reaches limit        ││ │ │ │
│ │ │ │  └─────────────────────┘│ │ │ │
│ │ │ └─────────────────────────┘ │ │ │
│ │ └─────────────────────────────┘ │ │
└──────────────────────────────────────┘
                     │
                     │
                     ▼
┌──────────────────────────────────────┐
│            OOM Condition              │
│ ┌──────────────────────────────────┐ │
│ │     Memory Demand Exceeds         │ │
│ │     Available Memory on a         │</code></pre></div>
<blockquote>
<p>1 / 1</p>
<p>继续</p>
</blockquote>
<p>1 / 1</p>
<p>│ │ Specific NUMA Node │ │ │ └──────────────────────────────────┘ │ └──────────────────────────────────────┘ │ │ ▼ ┌──────────────────────────────────────┐ │ OOM Killer │ │ ┌──────────────────────────────────┐ │ │ │ Selects and terminates │ │ │ │ processes to free up │ │ │ │ memory on specific NUMA node │ │ │ └──────────────────────────────────┘ │ └──────────────────────────────────────┘</p>
<p>objectivec</p>
<div><pre class="hljs"><code>在这个示例中，物理内存被划分为两个 NUMA 节点，并使用 <span class="hljs-built_in">CGroup</span> 机制将进程分组。每个进程组都有一个内存限制。当进程组中的某个进程尝试分配大量内存时，可能会导致该 NUMA 节点的物理内存耗尽，从而触发 OOM 状态。OOM Killer 组件会选择并终止一个或多个进程，以释放内存资源。在 NUMA 系统中，OOM Killer 可能会选择终止特定 NUMA 节点上的进程，以便在其他节点上分配更多的内存。

请注意，这个示例仅展示了 NUMA、<span class="hljs-built_in">CGroup</span> 和 OOM 之间关系的基本概念。实际上，这些组件之间的关系可能会更加复杂，并涉及到更多的细节。如果您需要进一步的解释或帮助，请随时提问。</code></pre></div>
<blockquote>
<p>1 / 1</p>
<p>请帮我画出oom的kill过程</p>
</blockquote>
<p>1 / 1</p>
<p>当系统进入 OOM 状态时，Linux 内核会尝试释放一些内存资源，以避免系统崩溃。如果释放内存资源无法解决问题，则内核会选择并终止一个或多个进程，以释放更多的内存资源。以下是一个简单的字符图，说明了 OOM Killer 组件选择进程的过程：</p>
<p>bash</p>
<div><pre class="hljs"><code>┌──────────────────────────────────────┐
│            OOM Killer                 │
│ ┌──────────────────────────────────┐ │
│ │   Determines <span class="hljs-built_in">which</span> processes      │ │
│ │   to <span class="hljs-built_in">kill</span> based on OOM score      │ │
│ └──────────────────────────────────┘ │
│ ┌──────────────────────────────────┐ │
│ │        OOM Score Formula         │ │
│ │  ┌───────────────────────────┐   │ │
│ │  │   process_score = base_score│   │ │
│ │  │        + adjustment        │   │ │
│ │  └───────────────────────────┘   │ │
│ └──────────────────────────────────┘ │
│ ┌──────────────────────────────────┐ │
│ │       Base OOM Score             │ │
│ │  ┌───────────────────────────┐   │ │
│ │  │         Priority          │   │ │
│ │  └───────────────────────────┘   │ │
│ └──────────────────────────────────┘ │
│ ┌──────────────────────────────────┐ │
│ │       Adjustment Factors         │ │
│ │  ┌───────────────────────────┐   │ │
│ │  │        Memory Usage       │   │ │
│ │  ├───────────────────────────┤   │ │
│ │  │     Age of Process        │   │ │
│ │  ├───────────────────────────┤   │ │
│ │  │      CPU Time Usage       │   │ │
│ │  ├───────────────────────────┤   │ │
│ │  │    Filesystem Pressure    │   │ │
│ │  ├───────────────────────────┤   │ │
│ │  │   Swapping/Paging Usage   │   │ │
│ │  └───────────────────────────┘   │ │
│ └──────────────────────────────────┘ │
└──────────────────────────────────────┘</code></pre></div>
<p>在这个示例中，OOM Killer 组件会根据 OOM Score Formula 确定每个进程的 OOM 得分。该得分基于进程的基本得分（例如，进程优先级）和一些调整因素（例如，进程的内存使用情况、CPU 时间使用情况等）。OOM Killer 组件会根据进程的 OOM 得分选择并终止一个或多个进程，以释放更多的内存资源。</p>
<p>请注意，这个示例仅展示了 OOM Killer 组件选择进程的基本概念。实际上，OOM Killer 组件的实现可能会更加复杂，并涉及到更多的细节。如果您需要进一步的解释或帮助，请随时提问。</p>
<blockquote>
<p>1 / 1</p>
<p>OOM Score Formula是什么</p>
</blockquote>
<p>1 / 1</p>
<p>OOM Score Formula是一种用于计算每个进程的OOM得分的公式。这个公式基于许多因素，包括进程的内存使用、优先级、CPU使用等等，用于确定在发生OOM时应该终止哪些进程。</p>
<p>下面是一个常用的OOM Score Formula：</p>
<p>makefile</p>
<div><pre class="hljs"><code>process_score = base_score + adjustment</code></pre></div>
<p>其中，<code>base_score</code> 是一个基本得分，通常基于进程的优先级。例如，较高优先级的进程可能会获得更高的基本得分。<code>adjustment</code> 是一些调整因素的总和，例如：</p>
<ul>
<li>内存使用：进程使用的内存越多，其得分就越高。</li>
<li>进程年龄：较旧的进程可能会获得较高的得分。</li>
<li>CPU 时间使用：进程使用的 CPU 时间越多，其得分就越高。</li>
<li>文件系统压力：在文件系统压力高时，进程可能会获得较高的得分。</li>
<li>交换/分页使用：使用交换/分页的进程可能会获得较高的得分。</li>
</ul>
<p>每个调整因素都可能会影响进程的OOM得分，但它们的影响因素会根据具体情况而异。例如，在内存非常紧缺的情况下，内存使用可能是最重要的因素。</p>
<p>请注意，OOM Score Formula 的实现可能因操作系统版本和内核版本而异。如果您需要进一步的解释或帮助，请随时提问。</p>
<blockquote>
<p>1 / 1</p>
<p>请画出OOM Score Formula的执行流程</p>
</blockquote>
<p>1 / 1</p>
<p>OOM Score Formula 的执行流程通常由 Linux 内核中的 OOM Killer 组件处理。以下是一个简单的字符图，说明了 OOM Score Formula 的执行流程：</p>
<p>arduino</p>
<div><pre class="hljs"><code>┌──────────────────────────────────────┐
│            OOM Killer                 │
│ ┌──────────────────────────────────┐ │
│ │      Calculates OOM Score        │ │
│ │      <span class="hljs-keyword">for</span> each process <span class="hljs-keyword">using</span>      │ │
│ │      OOM Score Formula           │ │
│ └──────────────────────────────────┘ │
│ ┌──────────────────────────────────┐ │
│ │      OOM Score Formula           │ │
│ │  ┌───────────────────────────┐   │ │
│ │  │   process_score = base_score│   │ │
│ │  │        + adjustment        │   │ │
│ │  └───────────────────────────┘   │ │
│ └──────────────────────────────────┘ │
│ ┌──────────────────────────────────┐ │
│ │      Base Score Calculation     │ │
│ │  ┌───────────────────────────┐   │ │
│ │  │         Priority          │   │ │
│ │  └───────────────────────────┘   │ │
│ └──────────────────────────────────┘ │
│ ┌──────────────────────────────────┐ │
│ │       Adjustment Factors         │ │
│ │  ┌───────────────────────────┐   │ │
│ │  │        Memory Usage       │   │ │
│ │  ├───────────────────────────┤   │ │
│ │  │     Age of <span class="hljs-built_in">Process</span>        │   │ │
│ │  ├───────────────────────────┤   │ │
│ │  │      CPU Time Usage       │   │ │
│ │  ├───────────────────────────┤   │ │
│ │  │    Filesystem Pressure    │   │ │
│ │  ├───────────────────────────┤   │ │
│ │  │   Swapping/Paging Usage   │   │ │
│ │  └───────────────────────────┘   │ │
│ └──────────────────────────────────┘ │
└──────────────────────────────────────┘</code></pre></div>
<p>在这个示例中，OOM Killer 组件为每个进程计算 OOM 得分。这个得分是通过将进程的基本得分（例如进程优先级）与调整因素相加而得出的。调整因素可以是任何可能影响进程 OOM 得分的因素，例如内存使用、CPU 时间使用、文件系统压力等等。</p>
<p>请注意，OOM Score Formula 的实现可能因操作系统版本和内核版本而异。如果您需要进一步的解释或帮助，请随时提问。</p>
<blockquote>
<p>1 / 1</p>
<p>请画出Linux 4.x版本的OOM Score Formula执行流程, 最好是流程图的方式</p>
</blockquote>
<p>1 / 1</p>
<p>以下是一个流程图，展示了 Linux 4.x 版本中 OOM Score Formula 的执行流程：</p>
<p>sql</p>
<pre><code>         ```sql
</code></pre>
<p>+--------------------+<br />
|                    |<br />
| OOM Killer         |<br />
|                    |<br />
+----------+---------+<br />
|<br />
|<br />
|<br />
+-----------v-----------+<br />
|                       |<br />
| Calculate OOM Score   |<br />
| for each process      |<br />
| using OOM Score       |<br />
| Formula               |<br />
|                       |<br />
+-----------+-----------+<br />
|<br />
|<br />
|<br />
+-----------v-----------+<br />
|                       |<br />
| Calculate Base Score  |<br />
| for each process      |<br />
| based on process      |<br />
| priority              |<br />
|                       |<br />
+-----------+-----------+<br />
|<br />
|<br />
|<br />
+-----------v-----------+<br />
|                       |<br />
| Calculate Adjustment  |<br />
| Factor for each       |<br />
| process               |<br />
|                       |<br />
+-----------+-----------+<br />
|<br />
|<br />
|<br />
+-----------v-----------+<br />
|                       |<br />
| Sum Base Score and    |<br />
| Adjustment Factor to  |<br />
| calculate OOM Score   |<br />
| for each process      |<br />
|                       |<br />
+-----------+-----------+<br />
|<br />
|<br />
|<br />
+-----------v-----------+<br />
|                       |<br />
| Select and kill       |<br />
| process with highest  |<br />
| OOM Score until       |<br />
| enough memory freed   |<br />
|                       |<br />
+-----------------------+</p>
<div><pre class="hljs"><code>
在这个示例中，OOM Killer 组件首先计算每个进程的 OOM 得分，然后选择并终止得分最高的进程，以释放内存资源。OOM 得分计算由三个步骤组成：

<span class="hljs-number">1.</span>  计算基本得分：为每个进程计算一个基本得分，基于进程的优先级。
<span class="hljs-number">2.</span>  计算调整因素：为每个进程计算一组调整因素，例如进程的内存使用情况、CPU 时间使用情况等。
<span class="hljs-number">3.</span>  计算 OOM 得分：将进程的基本得分与调整因素相加，得出 OOM 得分。

完成 OOM 得分计算后，OOM Killer 组件会选择并终止一个或多个进程，以释放内存资源。它会选择 OOM 得分最高的进程进行终止，直到释放足够的内存资源。

请注意，这个示例仅展示了 OOM Killer 组件选择进程的基本概念。实际上，OOM Killer 组件的实现可能会更加复杂，并涉及到更多的细节。如果您需要进一步的解释或帮助，请随时提问。

&gt; <span class="hljs-number">1</span> / <span class="hljs-number">1</span>
&gt; 
&gt; 可以将得分的因素都画出来嘛

<span class="hljs-number">1</span> / <span class="hljs-number">1</span>

当 Linux 内核的 OOM Killer 组件计算进程的 OOM 得分时，它会考虑多个因素，例如进程的内存使用、CPU 时间使用、文件系统压力等等。以下是一个简单的字符图，说明了 OOM Killer 组件计算 OOM 得分时考虑的因素：

arduino

```arduino
┌──────────────────────────────────────┐
│            OOM Killer                 │
│ ┌──────────────────────────────────┐ │
│ │   Determines OOM Score <span class="hljs-keyword">for</span> <span class="hljs-keyword">each</span>   │ │
│ │   process based <span class="hljs-keyword">on</span> multiple       │ │
│ │   factors <span class="hljs-keyword">using</span> OOM Score Formula │ │
│ └──────────────────────────────────┘ │
│ ┌──────────────────────────────────┐ │
│ │      OOM Score Formula           │ │
│ │  ┌───────────────────────────┐   │ │
│ │  │   process_score = base_score│   │ │
│ │  │        + adjustment        │   │ │
│ │  └───────────────────────────┘   │ │
│ └──────────────────────────────────┘ │
│ ┌──────────────────────────────────┐ │
│ │       Base Score Calculation     │ │
│ │  ┌───────────────────────────┐   │ │
│ │  │         Priority          │   │ │
│ │  └───────────────────────────┘   │ │
│ └──────────────────────────────────┘ │
│ ┌──────────────────────────────────┐ │
│ │       Adjustment Factors         │ │
│ │  ┌───────────────────────────┐   │ │
│ │  │        Memory <span class="hljs-keyword">Usage</span>       │   │ │
│ │  ├───────────────────────────┤   │ │
│ │  │     Age <span class="hljs-keyword">of</span> Process        │   │ │
│ │  ├───────────────────────────┤   │ │
│ │  │      CPU <span class="hljs-type">Time</span> <span class="hljs-keyword">Usage</span>       │   │ │
│ │  ├───────────────────────────┤   │ │
│ │  │    Filesystem Pressure    │   │ │
│ │  ├───────────────────────────┤   │ │
│ │  │   Swapping/Paging <span class="hljs-keyword">Usage</span>   │   │ │
│ │  └───────────────────────────┘   │ │
│ └──────────────────────────────────┘ │
└──────────────────────────────────────┘</code></pre></div>
<p>在这个示例中，OOM Killer 组件使用 OOM Score Formula 计算进程的 OOM 得分。这个得分由一个基本得分和一些调整因素相加而成。调整因素包括：</p>
<ul>
<li>内存使用：进程使用的内存越多，其得分就越高。</li>
<li>进程年龄：较旧的进程可能会获得较高的得分。</li>
<li>CPU 时间使用：进程使用的 CPU 时间越多，其得分就越高。</li>
<li>文件系统压力：在文件系统压力高时，进程可能会获得较高的得分。</li>
<li>交换/分页使用：使用交换/分页的进程可能会获得较高的得分。</li>
</ul>
<p>请注意，OOM Killer 组件计算 OOM 得分时使用的因素和权重可能因操作系统版本</p>
<blockquote>
<p>1 / 1</p>
<p>请画出OOM Score Formula的计算公式</p>
</blockquote>
<p>1 / 1</p>
<p>OOM Score Formula 是一种用于计算每个进程的 OOM 得分的公式。以下是一个简单的字符图，展示了 OOM Score Formula 的计算公式：</p>
<p>makefile</p>
<div><pre class="hljs"><code>process_score = base_score + adjustment</code></pre></div>
<p>在这个示例中，<code>process_score</code> 是一个进程的 OOM 得分，<code>base_score</code> 是一个基本得分，通常基于进程的优先级。例如，较高优先级的进程可能会获得更高的基本得分。<code>adjustment</code> 是一些调整因素的总和，例如：</p>
<ul>
<li>内存使用：进程使用的内存越多，其得分就越高。</li>
<li>进程年龄：较旧的进程可能会获得较高的得分。</li>
<li>CPU 时间使用：进程使用的 CPU 时间越多，其得分就越高。</li>
<li>文件系统压力：在文件系统压力高时，进程可能会获得较高的得分。</li>
<li>交换/分页使用：使用交换/分页的进程可能会获得较高的得分。</li>
</ul>
<p>每个调整因素都可能会影响进程的 OOM 得分，但它们的影响因素会根据具体情况而异。例如，在内存非常紧缺的情况下，内存使用可能是最重要的因素。</p>
<p>请注意，OOM Score Formula 的实现可能因操作系统版本和内核版本而异。如果您需要进一步的解释或帮助，请随时提问。</p>
<blockquote>
<p>1 / 1</p>
<p>请把公式列的更详细些</p>
</blockquote>
<p>1 / 1</p>
<p>OOM Score Formula 的公式由两个部分组成：基本得分和调整因素。以下是一个更详细的公式：</p>
<p>makefile</p>
<div><pre class="hljs"><code>process_score = base_score + adjustment

<span class="hljs-section">where:</span>

base_score = -1000 + (process_priority * (1000 / max_priority))

<span class="hljs-section">and:</span>

adjustment = memory_factor + age_factor + cpu_factor + fs_factor + swap_factor</code></pre></div>
<p>在这个公式中，<code>process_score</code> 是进程的 OOM 得分。<code>base_score</code> 是基于进程的优先级计算出的基本得分，<code>adjustment</code> 是多个调整因素的总和。每个调整因素都是通过计算一个因素得分并将其添加到 <code>adjustment</code> 中得出的。</p>
<p>其中，<code>process_priority</code> 是进程的优先级，<code>max_priority</code> 是系统中最高的优先级。<code>memory_factor</code> 表示内存使用因素的得分，<code>age_factor</code> 表示进程年龄因素的得分，<code>cpu_factor</code> 表示 CPU 时间使用因素的得分，<code>fs_factor</code> 表示文件系统压力因素的得分，<code>swap_factor</code> 表示交换/分页使用因素的得分。</p>
<p>请注意，不同的 Linux 内核版本可能会使用不同的 OOM Score Formula 实现。此外，每个因素的得分和权重可能因操作系统版本和内核版本而异。如果您需要进一步的解释或帮助，请随时提问。</p>
<blockquote>
<p>1 / 1</p>
<p>请将factor的因素也画出来</p>
</blockquote>
<p>1 / 1</p>
<p>当 Linux 内核的 OOM Killer 组件计算进程的 OOM 得分时，它会考虑多个因素。以下是一个简单的字符图，说明了 OOM Killer 组件计算 OOM 得分时考虑的因素以及对应的因素得分计算公式：</p>
<p>scss</p>
<div><pre class="hljs"><code>┌──────────────────────────────────────┐
│           OOM Score Factors           │
│  ┌──────────────────────────────┐    │
│  │     Memory Usage Factor      │    │
│  │  (higher usage -&gt; higher score)│    │
│  │  memory_factor = mem_used /    │    │
│  │  mem_total * 1000             │    │
│  └──────────────────────────────┘    │
│  ┌──────────────────────────────┐    │
│  │      Age of Process Factor   │    │
│  │ (older process -&gt; higher score)│    │
│  │  age_factor = (<span class="hljs-number">1.0</span> -         │    │
│  │  (now - start_time) /         │    │
│  │  (now - oldest_start_time))) │    │
│  └──────────────────────────────┘    │
│  ┌──────────────────────────────┐    │
│  │      CPU <span class="hljs-selector-tag">Time</span> Usage Factor   │    │
│  │ (more CPU time -&gt; higher score)│    │
│  │  cpu_factor = cpu_usage /     │    │
│  │  total_cpu * 1000             │    │
│  └──────────────────────────────┘    │
│  ┌──────────────────────────────┐    │
│  │     Filesystem Pressure      │    │
│  │ (higher pressure -&gt; higher score)│ │
│  │  fs_factor = (<span class="hljs-number">1.0</span> -           │    │
│  │  (fs_pressure / <span class="hljs-number">100</span>))         │    │
│  └──────────────────────────────┘    │
│  ┌──────────────────────────────┐    │
│  │     Swapping/Paging Usage    │    │
│  │ (more swap usage -&gt; higher score) │
│  │  swap_factor = swap_used /    │    │
│  │  swap_total * 1000            │    │
│  └──────────────────────────────┘    │
└──────────────────────────────────────┘</code></pre></div>
<p>在这个示例中，每个因素都有一个得分计算公式，以计算它对进程的 OOM 得分的贡献。例如，<code>memory_factor</code> 表示进程的内存使用情况的得分，可以通过计算 <code>mem_used / mem_total * 1000</code> 来计算。其中，<code>mem_used</code> 表示进程当前使用的内存量，<code>mem_total</code> 表示系统中可用的内存总量。</p>
<p>请注意，不同的操作系统版本和内核版本可能会使用不同的因素和权重计算 OOM 得分。此外，对于某些因素，如文件系统压力和交换/分页使用，计算公式可能会更加复杂。如果您需要进一步的解释或帮助，请随时提问。</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OOM]]></title>
            <guid>f37eae6d84674ed88bac2e2b754b9934</guid>
            <pubDate>Wed, 19 Apr 2023 13:56:43 GMT</pubDate>
            <content:encoded><![CDATA[<h3 id="源码文件">源码文件</h3>
<ul>
<li><code>mm/oom_kill.c</code>: 这个文件包含了 OOM killer 的主要实现代码，包括选择要杀死的进程、计算进程的 OOM 分数等功能。</li>
<li><code>include/linux/oom.h</code>: 这个文件包含了 OOM 相关的数据结构定义，例如 struct oom_control，以及 OOM 相关的一些宏定义。</li>
<li><code>mm/page_alloc.c</code>: 这个文件包含了内存分配的代码，包括内存分配失败时触发 OOM 的逻辑。</li>
<li><code>mm/vmscan.c</code>: 这个文件包含了内核中的内存回收机制，其中也涉及了在内存压力过大时触发 OOM 的部分逻辑。</li>
<li><code>include/linux/memcontrol.h</code> 和 <code>mm/memcontrol.c</code>: 这两个文件包含了内存 cgroup（控制组）的实现。OOM killer 也可以在 cgroup 内触发，以便在特定的 cgroup 内释放内存。</li>
<li><code>kernel/sysctl.c</code>: 这个文件包含了内核参数（sysctl）的实现，其中包括 OOM 相关的参数，例如调整 OOM killer 的启用或禁用。</li>
<li><code>fs/proc/base.c</code> 和 <code>fs/proc/task_mmu.c</code>: 这两个文件包含了 /proc 文件系统中与 OOM 相关的信息的实现，例如 /proc/<pid class="jop-noMdConv">/oom_score 和 /proc/<pid class="jop-noMdConv">/oom_score_adj。</pid></pid></li>
</ul>
<h3 id="oom相关的数据结构">OOM相关的数据结构</h3>
<p>在 Linux 内核中，内存管理是一个相当复杂的过程，涉及多个数据结构。为了简化问题并更好地理解与 OOM（Out-of-Memory）相关的内存结构，我们将关注以下几个主要组件：</p>
<ol>
<li>mm_struct：这个结构表示一个进程的内存描述符。它包含了与虚拟内存区域（VMAs）有关的信息以及其他内存管理相关的数据。</li>
<li>vm_area_struct：这个结构表示一个虚拟内存区域，它在进程地址空间中定义了一个连续的虚拟内存范围。每个 VMA 都与一个 mm_struct 实例相关联。</li>
<li>page：这个结构表示物理内存中的一个页面。Linux 内核使用 page 结构来跟踪物理内存的使用情况。</li>
<li>mem_cgroup：内存控制组(memcg)是一种用于将内存资源划分给不同任务的机制。mem_cgroup 结构表示一个内存控制组，其中包含与该组相关的内存使用信息。</li>
</ol>
<p>以字符图的形式表示，这些结构可以如下所示：</p>
<div><pre class="hljs"><code>mm_struct
├─── VMAs (vm_area_struct)
│    ├─── Start Address
│    ├─── End Address
│    ├─── Flags (e.g., Read, Write, Execute)
│    └─── Anon/Private <span class="hljs-keyword">or</span> <span class="hljs-keyword">File</span>/Shared pages
│
├─── Page Tables
│    └─── Mapping of Virtual Addresses to Physical Pages
│
└─── Other memory management related fields
     ├─── rss_stat (Resident <span class="hljs-keyword">Set</span> Size <span class="hljs-comment">Statistics)</span>
     ├─── mmap_sem <span class="hljs-comment">(Semaphore for mmap operations)</span>
     └─── ...

page
├─── Physical <span class="hljs-comment">Address</span>
├─── Flags <span class="hljs-comment">(e.g., Dirty, Accessed)</span>
├─── Reference <span class="hljs-comment">Counter</span>
└─── LRU <span class="hljs-comment">(Least Recently Used) List</span>

mem_cgroup
├─── Memory <span class="hljs-comment">Usage (per cgroup)</span>
├─── Memory <span class="hljs-comment">Limit (per cgroup)</span>
├─── Soft <span class="hljs-comment">Limit (per cgroup)</span>
└─── OOM <span class="hljs-comment">Control (OOM killer settings)</span></code></pre></div>
<h3 id="oom出现的原因">OOM出现的原因</h3>
<p>在 Linux 系统中，当可用内存资源不足以满足当前进程的内存需求时，就会触发 Out-of-Memory (OOM) 状态。OOM 的发生可能由多种原因引起，例如内存泄漏、资源竞争或配置错误。以下是一个简化的字符图，说明了导致 OOM 状态的一些关键因素：</p>
<div><pre class="hljs"><code>┌────────────────────────────┐
│       System <span class="hljs-keyword">Memory</span>         │
│ ┌────────────────────────┐ │
│ │      Free <span class="hljs-keyword">Memory</span>        │ │
│ └────────────────────────┘ │
│ ┌────────────────────────┐ │
│ │   <span class="hljs-keyword">Memory</span> Cgroup 1       │ │
│ │ ┌─────────────────────┐ │ │
│ │ │  Process A          │ │ │
│ │ │  ┌─────────────────┐│ │ │
│ │ │  │ Allocates <span class="hljs-keyword">more</span>  ││ │ │
│ │ │  │ <span class="hljs-keyword">memory</span> than     ││ │ │
│ │ │  │ available or     ││ │ │
│ │ │  │ reaches limit    ││ │ │
│ │ │  └─────────────────┘│ │ │
│ │ └─────────────────────┘ │ │
│ └────────────────────────┘ │
│ ┌────────────────────────┐ │
│ │   <span class="hljs-keyword">Memory</span> Cgroup 2       │ │
│ │ ┌─────────────────────┐ │ │
│ │ │  Process B          │ │ │
│ │ │  ┌─────────────────┐│ │ │
│ │ │  │ Allocates <span class="hljs-keyword">more</span>  ││ │ │
│ │ │  │ <span class="hljs-keyword">memory</span> than     ││ │ │
│ │ │  │ available or     ││ │ │
│ │ │  │ reaches limit    ││ │ │
│ │ │  └─────────────────┘│ │ │
│ │ └─────────────────────┘ │ │
│ └────────────────────────┘ │
└────────────────────────────┘
</code></pre></div>
<p>在这个示例中，系统内存被划分为多个内存控制组（Memory Cgroup）。当进程 A 或进程 B 尝试分配比其内存控制组可用的更多内存时，就会导致 OOM 状态。</p>
<p>OOM Killer 是 Linux 内核中的一个组件，当系统检测到内存不足时，它会根据一系列标准（如进程的内存使用、运行时间、优先级等）来选择并终止一个或多个进程，以释放内存资源。</p>
<p>这个示例仅是为了说明 OOM 的基本概念，实际上可能涉及更多的因素。如果您需要进一步的解释或帮助，请随时提问。</p>
<h3 id="虚拟内存物理内存和oom的关系">虚拟内存，物理内存和OOM的关系</h3>
<p>以下图展示了虚拟内存、物理内存和 OOM 之间的关系。在这个示例中，Process A 尝试分配虚拟内存，该内存将映射到物理内存。当物理内存耗尽时，会触发 OOM 状态。</p>
<div><pre class="hljs"><code>┌────────────────────────────────────┐
│       Virtual Memory (Process A)    │
│ ┌────────────────────────────────┐ │
│ │   Virtual Memory<span class="hljs-built_in"> Area </span>(VMA)    │ │
│ │  ┌────────────────────────────┐│ │
│ │  │ Virtual<span class="hljs-built_in"> Address </span>           ││ │
│ │  └────────────────────────────┘│ │
│ └────────────────────────────────┘ │
└────────────────────────────────────┘
                     │
                     │
                     ▼
┌────────────────────────────┐
│       <span class="hljs-built_in"> Page </span>Tables          │
│ ┌────────────────────────┐ │
│ │  Virtual<span class="hljs-built_in"> Address </span>       │ │
│ │  └───┬─────────────────┘ │
│ └──────┴──────────────────┘ │
└────────────────────────────┘
                     │
                     │
                     ▼
┌────────────────────────────┐
│       Physical Memory      │
│ ┌────────────────────────┐ │
│ │  Physical<span class="hljs-built_in"> Page </span>1        │ │
│ └────────────────────────┘ │
│ ┌────────────────────────┐ │
│ │  Physical<span class="hljs-built_in"> Page </span>2        │ │
│ └────────────────────────┘ │
│          <span class="hljs-built_in">..</span>.               │
│ ┌────────────────────────┐ │
│ │  Physical<span class="hljs-built_in"> Page </span>N        │ │
│ └────────────────────────┘ │
└────────────────────────────┘
                     │
                     │
                     ▼
┌────────────────────────────┐
│           OOM              │
│ ┌────────────────────────┐ │
│ │ <span class="hljs-keyword">Not</span> enough physical     │ │
│ │ memory <span class="hljs-keyword">to</span> fulfill       │ │
│ │ virtual memory request  │ │
│ └────────────────────────┘ │
└────────────────────────────┘</code></pre></div>
<p>在这个示例中，Process A 的虚拟内存被划分为一个或多个虚拟内存区域（VMA）。虚拟地址属于这些区域。Page Tables 负责将虚拟地址映射到物理内存中的实际页面。当进程尝试分配虚拟内存时，Linux 内核会分配对应的物理页面。然而，当物理内存耗尽以至于无法分配足够的物理页面时，将触发 OOM 状态。这可能导致 OOM Killer 组件选择并终止一个或多个进程，以释放内存资源。</p>
<p>这个示例仅展示了虚拟内存、物理内存和 OOM 之间关系的基本概念。实际上，Linux 内核使用多级页表来进行映射，并涉及到更多的细节。</p>
<h3 id="numa-cgroup和oom的关系">NUMA, CGROUP和OOM的关系</h3>
<p>在 Linux 系统中，CGroup 是一种机制，用于限制进程对系统资源（如 CPU、内存、磁盘等）的访问。在 NUMA 系统中，CGroup 还可以用于管理进程对不同 NUMA 节点上的内存的访问。以下是一个字符图，说明了 NUMA、CGroup 和 OOM 之间的关系：</p>
<div><pre class="hljs"><code>┌──────────────────────────────────────┐
│            NUMA Architecture          │
│ ┌──────────────────────────────────┐ │
│ │           NUMA <span class="hljs-keyword">Node</span> <span class="hljs-title">1</span>            │ │
│ │  ┌────────────────────────────┐  │ │
│ │  │       Physical Memory       │  │ │
│ │  └────────────────────────────┘  │ │
│ │  ┌────────────────────────────┐  │ │
│ │  │        CPU Cores            │  │ │
│ │  └────────────────────────────┘  │ │
│ │ ┌─────────────────────────────┐ │ │
│ │ │       Memory Cgroup <span class="hljs-number">1</span>       │ │ │
│ │ │ ┌─────────────────────────┐ │ │ │
│ │ │ │  Process A              │ │ │ │
│ │ │ │  ┌─────────────────────┐│ │ │ │
│ │ │ │  │ Allocates more memory││ │ │ │
│ │ │ │  │ than available <span class="hljs-keyword">or</span>    ││ │ │ │
│ │ │ │  │ reaches limit        ││ │ │ │
│ │ │ │  └─────────────────────┘│ │ │ │
│ │ │ └─────────────────────────┘ │ │ │
│ │ └─────────────────────────────┘ │ │
│ └──────────────────────────────────┘ │
│ ┌──────────────────────────────────┐ │
│ │           NUMA <span class="hljs-keyword">Node</span> <span class="hljs-title">2</span>            │ │
│ │  ┌────────────────────────────┐  │ │
│ │  │       Physical Memory       │  │ │
│ │  └────────────────────────────┘  │ │
│ │  ┌────────────────────────────┐  │ │
│ │  │        CPU Cores            │  │ │
│ │  └────────────────────────────┘  │ │
│ │ ┌─────────────────────────────┐ │ │
│ │ │       Memory Cgroup <span class="hljs-number">2</span>       │ │ │
│ │ │ ┌─────────────────────────┐ │ │ │
│ │ │ │  Process B              │ │ │ │
│ │ │ │  ┌─────────────────────┐│ │ │ │
│ │ │ │  │ Allocates more memory││ │ │ │
│ │ │ │  │ than available <span class="hljs-keyword">or</span>    ││ │ │ │
│ │ │ │  │ reaches limit        ││ │ │ │
│ │ │ │  └─────────────────────┘│ │ │ │
│ │ │ └─────────────────────────┘ │ │ │
│ │ └─────────────────────────────┘ │ │
└──────────────────────────────────────┘
                     │
                     │
                     ▼
┌──────────────────────────────────────┐
│            OOM Condition              │
│ ┌──────────────────────────────────┐ │
│ │     Memory Demand Exceeds         │ │
│ │     Available Memory on a         │
│ │ Specific NUMA <span class="hljs-keyword">Node</span> <span class="hljs-title">│ │
│ └──────────────────────────────────┘ │
└──────────────────────────────────────┘
│
│
▼
┌──────────────────────────────────────┐
│ OOM</span> Killer │
│ ┌──────────────────────────────────┐ │
│ │ Selects <span class="hljs-keyword">and</span> terminates │ │
│ │ processes to free up │ │
│ │ memory on specific NUMA <span class="hljs-keyword">node</span> <span class="hljs-title">│ │
│ └──────────────────────────────────┘ │
└──────────────────────────────────────┘</span></code></pre></div>
<h3 id="模拟oom事件">模拟OOM事件</h3>
<p>好的，以下是模拟在CGROUP中发生OOM的步骤：</p>
<ol>
<li>
<p>确认系统已启用cgroup v2，如果没有启用，可以在内核编译时配置或者在运行时使用命令行参数启用。</p>
</li>
<li>
<p>创建一个cgroup，例如：<code>sudo mkdir /sys/fs/cgroup/memory/mycgroup</code></p>
</li>
<li>
<p>设置cgroup的限制，例如：<code>echo 100M &gt; /sys/fs/cgroup/memory/mycgroup/memory.limit_in_bytes</code></p>
</li>
<li>
<p>启动一个占用内存的进程并将其加入到cgroup中，例如：<code>sudo cgexec -g memory:mycgroup stress --vm-bytes 200M</code></p>
</li>
<li>
<p>监控cgroup内存使用情况，例如：<code>watch -n 1 "cat /sys/fs/cgroup/memory/mycgroup/memory.usage_in_bytes"</code></p>
</li>
<li>
<p>当进程占用的内存超过cgroup的限制时，就会发生OOM事件。可以通过检查内核日志或者<code>dmesg</code>命令查看具体信息。在cgroup v2中，系统会杀死整个cgroup中的所有进程以释放内存。</p>
</li>
<li>
<p>如果需要更加详细的OOM事件记录和处理，可以使用一些工具，例如<code>systemd-oomd</code>或者<code>oomd</code>。这些工具可以更加精细地控制和记录OOM事件，并提供更多的处理选项。</p>
</li>
</ol>
<h3 id="swap分区">Swap分区</h3>
<div><pre class="hljs"><code>   +----------------+              +----------------+              +----------------+              +----------------+
   |   Physical     |  Read/Write  |   Swap Space   |  Read/Write  | Virtual Memory |  Read/Write  |  Physical Memory|  Read/Write
   |    Devices     |-------------&gt;|                |-------------&gt;|                |-------------&gt;|                |
   +----------------+              +----------------+              +----------------+              +----------------+
</code></pre></div>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[共享内存]]></title>
            <guid>3dcbaa4cae934e679ad1d58e28b6642e</guid>
            <pubDate>Wed, 19 Apr 2023 13:56:10 GMT</pubDate>
            <content:encoded><![CDATA[<h3 id="资料">资料</h3>
<p>要学习 Linux 共享内存（Shared Memory）相关的内容，可以查看以下源代码文件：</p>
<ul>
<li>
<p>ipc/shm.c: 这个文件包含了 System V 共享内存实现的主要代码。System V 共享内存是 Linux 中最早的共享内存实现。</p>
</li>
<li>
<p>ipc/util.c: 这个文件包含了 IPC（包括共享内存）相关的辅助功能和数据结构。</p>
</li>
<li>
<p>include/linux/shm.h: 这个文件包含了 System V 共享内存的一些常量、宏和结构体定义。</p>
</li>
<li>
<p>include/uapi/linux/shm.h：该文件包含了与 System V 共享内存相关的用户空间 API 定义。</p>
</li>
<li>
<p>mm/mmap.c: 这个文件包含了内存映射（mmap）的实现代码。内存映射可以用于创建共享内存区域，例如使用 MAP_SHARED 标志。mmap 也是实现 POSIX 共享内存的基础。</p>
</li>
<li>
<p>include/linux/mman.h: 这个文件包含了内存映射相关的宏、常量和结构体定义。</p>
</li>
<li>
<p>fs/proc/task_mmu.c: 这个文件包含了 /proc 文件系统中与共享内存相关的信息的实现，例如 /proc/<pid class="jop-noMdConv">/maps。</pid></p>
</li>
<li>
<p>include/linux/shmem_fs.h：该文件包含了与 shmem 文件系统相关的内核空间定义。</p>
</li>
<li>
<p>fs/hugetlbfs/inode.c: 这个文件包含了 hugetlbfs 的实现，hugetlbfs 可以用于创建大页（huge pages）的共享内存区域。</p>
</li>
</ul>
<p>除了 System V 共享内存，Linux 还支持 POSIX 共享内存。要了解 POSIX 共享内存，你可能需要查看以下文件：</p>
<ul>
<li>mm/shmem.c：此文件包含shmem文件系统的实现，它为POSIX共享内存提供了基础设施。</li>
<li>fs/proc_namespace.c：此文件包含/proc文件系统的命名空间实现，其中与POSIX共享内存相关的部分也在这个文件中。</li>
<li>ipc/shm.c：此文件包含System V共享内存实现，虽然与POSIX共享内存不完全相同，但它们之间存在一定程度的相似性和互操作性。</li>
</ul>
<p>学习这些文件将帮助你了解 Linux 共享内存的实现原理、操作方式和与其他系统组件的交互。此外，建议先阅读关于共享内存的概念性文章和文档，以便了解基本概念和原理，然后再深入阅读源代码。</p>
<h3 id="内存种类">内存种类</h3>
<p>System V 共享内存和 POSIX 共享内存都是在 Linux 中实现的进程间通信 (IPC) 机制，它们允许多个进程访问同一块物理内存，从而实现数据共享。尽管它们的目标相似，但它们在实现和使用方式上有一些区别：</p>
<ul>
<li>接口：System V 共享内存使用了一组特定的系统调用（如 shmget(), shmat(), shmdt() 和 shmctl()），而 POSIX 共享内存则使用了类似文件操作的系统调用（如 shm_open(), shm_unlink(), mmap() 和 munmap()）。</li>
<li>资源标识：System V 共享内存使用整数键值 (key) 和共享内存标识符 (shmid) 来标识共享内存对象。POSIX 共享内存使用类似于文件路径的名称来标识共享内存对象。</li>
<li>生命周期：System V 共享内存的生命周期与创建它的进程无关，即使创建它的进程退出，共享内存对象仍然存在，直到系统重启或显式删除。而 POSIX 共享内存的生命周期类似于文件，可以通过引用计数和删除操作来管理。</li>
<li>文件系统支持：System V 共享内存不需要文件系统支持，而 POSIX 共享内存通常基于 tmpfs 文件系统（或类似于 tmpfs 的文件系统）。</li>
<li>可移植性：System V 共享内存起源于早期的 UNIX 系统，因此在各种 UNIX 和类 UNIX 系统上的可移植性较好。POSIX 共享内存是基于 POSIX 标准的，因此在遵循 POSIX 标准的系统上具有更好的可移植性。</li>
</ul>
<p>适用场景：System V 共享内存在一些较早的应用程序中使用较多，而 POSIX 共享内存较为现代，更符合当前编程实践。</p>
<p>在选择使用 System V 共享内存还是 POSIX 共享内存时，需要根据实际应用场景和兼容性需求进行权衡。POSIX 共享内存在很多情况下是一个较为现代和灵活的选择。</p>
<h3 id="区别">区别</h3>
<p>设备共享内存和匿名共享内存是两种不同的内存类型，它们与 System V 共享内存和 POSIX 共享内存的关系如下：</p>
<ul>
<li>
<p>设备共享内存：设备共享内存通常用于在设备驱动程序和用户空间进程之间共享数据。这种共享内存类型通常与 DMA（直接内存访问）相关，以便硬件设备可以直接访问内存，而无需通过 CPU。设备共享内存的实现与 System V 共享内存和 POSIX 共享内存都不同，它通常依赖于特定的设备驱动程序和内核支持。</p>
</li>
<li>
<p>匿名共享内存：匿名共享内存是一种不需要文件系统支持的共享内存类型。它可以使用 mmap() 系统调用创建，传递 MAP_SHARED 和 MAP_ANONYMOUS 参数。在这种情况下，匿名共享内存更接近于 POSIX 共享内存，因为它们都依赖于 mmap() 系统调用。然而，匿名共享内存没有与文件系统相关联的名称，它的生命周期通常与引用它的进程相关。</p>
</li>
</ul>
<p>总之，设备共享内存与 System V 共享内存和 POSIX 共享内存都不同，而匿名共享内存在使用方法上更接近于 POSIX 共享内存，但不依赖于文件系统支持。</p>
<h3 id="总结">总结</h3>
<div><pre class="hljs"><code>共享内存 (Shared Memory)
|
|-- System V 共享内存 (System V Shared Memory)
|
|-- POSIX 共享内存 (POSIX Shared Memory)
|   |
|   |-- 匿名共享内存 (Anonymous Shared Memory)
|   |
|   `-- 设备共享内存 (Device Shared Memory)
|
`-- 特定硬件/库实现的共享内存 (Hardware/Library-specific Shared Memory)
    |
    |-- GPU 共享内存 (e.g., CUDA, OpenCL)
    |
    `-- 其他特定实现 (Other specific implementations)</code></pre></div>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[内存管理]]></title>
            <guid>326f6a784a8341049d9e6bb44a734024</guid>
            <pubDate>Wed, 12 Apr 2023 14:01:25 GMT</pubDate>
            <content:encoded><![CDATA[<h2 id="从oomout-of-memory开始">从OOM(Out Of Memory)开始</h2>
<blockquote>
<p>内存溢出（OOM，Out Of Memory）错误是指当内核在其内部内存池中耗尽内存且无法从其他来源回收内存时发生的情况。它基本上会开始随机终止进程，并在dmesg中生成大量日志。</p>
</blockquote>
<h3 id="导致内存溢出oom的原因是什么">导致内存溢出（OOM）的原因是什么？</h3>
<p>通常，系统在回收内存方面表现得相当懒散，更倾向于让内存留在缓存中，直到真正需要使用。因此，如果没有内存请求，看到内存使用量增长而不减少并不罕见。当有内存请求时，系统可能会选择释放一些无人使用的内存以满足请求，或者将仍在使用的数据放到交换空间中，然后交出现在可用的内存。如果交换空间上的数据再次被需要，它将替换一些其他不再使用的内存。当这种替换过程被认为已经停止取得进展时，就会发生内存溢出（OOM）。</p>
<p>当内存紧张时，整个进程会被终止，理论上这样做可以释放大量内存。这并不是一个完全理想的解决方案，但它确实（理论上）允许系统继续运行。然而在实践中，用户通常会反感他们的进程被强制终止，这通常也是问题引起关注的时候。</p>
<h3 id="导致这些内存溢出oom事件的原因是什么">导致这些内存溢出（OOM）事件的原因是什么？</h3>
<ul>
<li>内核确实内存不足, 工作负载使用的内存超过了系统的RAM和交换空间
<ul>
<li>在<code>/proc/meminfo</code>中，查看<code>SwapFree</code>和<code>MemFree</code>分别是什么？如果两者都非常低（小于总量的1%），那么工作负载可能有问题。（除非涉及到<code>mlock()</code>或<code>HugeTLB</code>，具体请见下文...)
<ul>
<li>SwapFree：表示系统中当前可用的交换空间大小，单位是KB</li>
<li>MemFree：表示系统中当前可用的物理内存大小，单位是KB</li>
</ul>
</li>
</ul>
</li>
<li>在32位架构上，内核的低内存耗尽了
<ul>
<li>在<code>/proc/meminfo</code>中，<code>LowFree</code>是什么？如果它非常低，但<code>HighFree</code>要高得多，那么你可能面临这种情况。这种工作负载在64位平台或内核上运行可能会有益处。</li>
</ul>
</li>
<li>存在内核数据结构或内存泄漏问题
<ul>
<li>在<code>/proc/meminfo</code>文件中，查看<code>SwapFree</code>和<code>MemFree</code>分别是什么？</li>
<li>在<code>slabinfo</code>中，<code>task_struct</code>对象的数量是多少？系统是否创建了如此多的进程，以至于耗尽了内存？
<ul>
<li>这里需要你查看<code>/proc/slabinfo</code>文件，找到<code>task_struct</code>对应的行，其中会显示当前系统中<code>task_struct</code>对象的数量。如果数量过多，可能是系统创建了过多进程，导致内存耗尽</li>
</ul>
</li>
<li>在/proc/slabinfo中，哪些对象占用了最多的空间？如果某种对象占据了系统总内存的绝大部分，那么这个对象可能是问题所在。要查看对象的使用情况，请在命令行上运行以下命令：
<ul>
<li><code>awk '{printf "%5d MB %s\n", $3*$4/(1024*1024), $1}' &lt; /proc/slabinfo | sort -n</code></li>
</ul>
</li>
<li>内核没有正确使用其交换空间
<ul>
<li>如果应用程序使用<code>mlock()</code>或<code>HugeTLBfs</code>页，则可能无法为该应用程序使用其交换空间。如果发生这种情况，当OOM发生时，<code>SwapFree</code>可能仍然有一个非常大的值。但是，这两个特性不允许系统交换受影响的内存，因此过度使用它们可能耗尽系统内存，使系统没有其他资源</li>
<li>系统也可能陷入一种类似死锁的状态。将数据写入磁盘本身可能需要为各种I/O数据结构分配内存。如果系统连这些内存都找不到，用于创建空闲内存的函数将受到限制，系统很可能会耗尽内存。可以通过进行一些微调来更早地开始分页，但是如果系统无法足够快地将脏页写出以释放内存，那么我们只能得出结论：工作负载与已安装的内存不匹配，而且没有太多可做的事情。提高<code>/proc/sys/vm/min_free_kbytes</code>中的值会使系统比以前更早地开始回收内存。这使得陷入这种死锁的难度更大。</li>
</ul>
</li>
<li>内核做出了错误的决策，并误读了其统计数据, 在还有大量可用的优质RAM的情况下，系统发生了内存溢出（OOM）</li>
<li>发生了一些非常病态的情况。实际上，在花费了“相当多”的时间扫描内存寻找可释放内容后，内核决定触发内存溢出（OOM),  如果内核正在快速扫描页面，但是您的I/O设备(交换、文件系统或网络fs)太慢，内核可能会判断没有任何进展，即使有交换空闲，也会触发OOM</li>
</ul>
</li>
</ul>
<h2 id="走进linux的内存演化路线">走进Linux的内存演化路线</h2>
<h3 id="发展历程">发展历程</h3>
<ol>
<li>
<p><strong>早期阶段(1991年-2001年):</strong>  在早期的Linux内核版本中，内存管理主要依赖于x86硬件架构的页表机制。在这个阶段，内存管理主要包括内存分配和回收、虚拟内存、页面置换等基本功能。早期的Linux内核使用了经典的Buddy System算法来管理物理内存，该算法可以高效地分配和回收大块物理内存。</p>
</li>
<li>
<p><strong>2.4内核阶段(2001年-2003年):</strong>  随着计算机技术的不断发展，内存容量也越来越大，早期的内存管理方案已经无法满足需求。因此，Linux内核开始引入一系列新的内存管理机制，例如Slab Allocator、NUMA和高速缓存等，以支持更大规模的内存管理。在2.4版本的内核中，Linux内存管理的设计和实现更加高效和灵活。</p>
</li>
<li>
<p><strong>2.6内核阶段(2003年-2011年):</strong>  2.6版本的内核是Linux内存管理的一个重要里程碑，它引入了许多新的特性，例如：内存压缩、大页面、回收和管理虚拟内存、Memory Controller等。此外，2.6内核还加入了可扩展的物理页框架（SPF）和完整性保护（Integrity Protection）等高级特性。</p>
</li>
<li>
<p><strong>3.x和4.x内核阶段(2011年-至今):</strong>  自3.x和4.x内核以来，Linux内存管理的重点已经转移到了能耗优化、虚拟化和NUMA等方面。新的特性包括Transparent Huge Pages、Control Group（Cgroup）等，可以帮助管理员更好地管理系统内存。同时，内核还引入了对无线电源管理、休眠模式和睡眠状态等特性的支持，以更好地满足现代计算机的需求。</p>
</li>
</ol>
<h3 id="linux各版本中涉及的重要特性和改进">Linux各版本中涉及的重要特性和改进</h3>
<ol>
<li>
<p>Linux 0.0.1 (1991年)：最初的Linux版本只支持1MB的物理内存，使用简单的伙伴系统（Buddy System）来管理物理内存。</p>
</li>
<li>
<p>Linux 1.0 (1994年)：支持2GB的物理内存，引入了虚拟内存和页面置换功能。</p>
</li>
<li>
<p>Linux 2.2 (1999年)：引入了高速缓存、共享内存和NUMA支持等特性，以支持更大规模的内存管理。</p>
</li>
<li>
<p>Linux 2.4 (2001年)：引入了Slab Allocator来管理内核对象的内存分配和回收，可以高效地处理小块内存。此外，2.4版本还支持大页面（Huge Pages）和内存压缩（Memory Compression）等特性。</p>
</li>
<li>
<p>Linux 2.6 (2003年)：引入了Memory Controller和Completely Fair Scheduler（CFS）等特性，可以更好地管理和调度系统内存和进程。此外，2.6版本还支持SPF和完整性保护（Integrity Protection）等高级特性。</p>
</li>
<li>
<p>Linux 3.x和4.x (2011年-至今)：引入了Transparent Huge Pages、Control Group（Cgroup）等特性，可以帮助管理员更好地管理系统内存。同时，内核还引入了对无线电源管理、休眠模式和睡眠状态等特性的支持。</p>
</li>
</ol>
<h3 id="源码分析">源码分析</h3>
<h4 id="linux-001">Linux 0.0.1</h4>
<p><a title="https://github.com/zavg/linux-0.01.git" href="https://github.com/zavg/linux-0.01.git">Linux 0.0.1</a></p>
<p>内存管理相关的文件：</p>
<ol>
<li><code>include/asm/memory.h</code>: 定义了<code>memcpy</code>函数</li>
<li><code>mm/memory.c</code>: 主要实现了<code>include/linux/mm.h</code>中<code>get_free_page</code>, <code>put_page</code>和<code>free_page</code>三个内存分配函数</li>
<li>在这一版的源码中已经出现了<code>HIGH_MEMORY</code>，<code>LOW_MEMORY</code>和<code>vmem_map</code>等概念</li>
</ol>
<h3 id="linux-10">Linux 1.0</h3>
<h3 id="linux-4x">LInux 4.x</h3>
<div><pre class="hljs"><code>+--------------------------------------+
|         用户态 (User space)           |
+--------------------------------------+
|         内核态 (Kernel space)         |
+---------+----------+-----------------+
| 物理内存 | 页表管理 | 高速缓存 &amp; 缓冲区  |
+---------+----------+-----------------+
|           硬件层 (Hardware)           |
+--------------------------------------+</code></pre></div>
<ul>
<li>用户态 (User space)：用户程序和应用程序运行的区域。</li>
<li>内核态 (Kernel space)：内核代码和数据结构所在的区域。</li>
<li>物理内存 (Physical memory)：用于存储内核、用户程序和数据的实际内存。</li>
<li>页表管理 (Page table management)：用于管理虚拟地址到物理地址的映射关系。</li>
<li>高速缓存 &amp; 缓冲区 (Cache &amp; Buffer)：用于加速内存访问和数据交换的缓存结构。</li>
<li>硬件层 (Hardware)：包括物理内存、CPU 和其他硬件设备。</li>
</ul>
<h4 id="虚拟内存">虚拟内存</h4>
<div><pre class="hljs"><code>+-----------------------------+
|      虚拟地址空间 (VAS)        |
+-------------+---------------+
|  用户虚拟地址 | 内核虚拟地址    |
+------+------+------+--------+
| 页全局目录 | 页上级目录 | 页目录 | 页表 |
+------+------+------+--------+
|       物理地址空间 (PAS)        |
+-----------------------------+</code></pre></div>
<ul>
<li>虚拟地址空间 (Virtual Address Space, VAS)：操作系统为每个进程分配的地址空间。</li>
<li>用户虚拟地址 (User virtual address)：用户程序和应用程序运行的虚拟地址区域。</li>
<li>内核虚拟地址 (Kernel virtual address)：内核代码和数据结构的虚拟地址区域。</li>
<li>页全局目录 (Page Global Directory)：最高级别的页表，用于指向页上级目录。</li>
<li>页上级目录 (Page Upper Directory)：用于指向页目录。</li>
<li>页目录 (Page Directory)：用于指向页表。</li>
<li>页表 (Page Table)：用于将虚拟地址映射到物理地址。</li>
<li>物理地址空间 (Physical Address Space, PAS)：实际内存的地址空间。</li>
</ul>
<div><pre class="hljs"><code>+------------------------------------+
|       虚拟地址空间 (VAS)              |
|+----------------------------------+|
||      用户虚拟地址 (UVA)            ||
||+------------+------------+------+||
||| 用户栈     | 用户数据    | 用户代码 |||
||+------------+------------+------+||
|+----------------------------------+|
|+----------------------------------+|
||      内核虚拟地址 (KVA)            ||
||+------------+------------+------+||
||| 内核栈     | 内核数据    | 内核代码 |||
||+------------+------------+------+||
|+----------------------------------+|
+------------------------------------+
</code></pre></div>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[限流算法]]></title>
            <guid>e877991b34564e5788dda7604a0d76af</guid>
            <pubDate>Wed, 12 Apr 2023 12:38:42 GMT</pubDate>
            <content:encoded><![CDATA[<blockquote>
<p>节流算法是用于控制应用程序或系统消耗资源或处理请求的速率的技术，有助于维护系统的稳定性，保证资源的公平分配，防止系统过载或崩溃。</p>
</blockquote>
<h3 id="常见的节流算法">常见的节流算法</h3>
<ol>
<li>令牌桶算法:该算法维护一个虚拟的令牌桶，在处理请求时使用这些令牌。令牌以固定的速率生成，如果没有令牌，请求要么被丢弃，要么被延迟，直到新的令牌可用为止。这种方法允许高请求率的短爆发，使总请求率随着时间的推移变得平稳。</li>
<li>漏桶算法:漏桶算法与令牌桶算法类似，也使用了虚拟桶。但是，它不使用令牌，而是累积传入的请求，并以恒定的速率处理它们。如果桶已满，新的传入请求将被丢弃或延迟。该算法执行严格的、一致的请求率。</li>
<li>固定窗口速率限制:该算法将时间划分为固定的窗口(例如，每秒、分钟或小时)，并允许每个窗口内的最大请求数量。如果达到限制，新的请求要么被丢弃，要么被延迟，直到下一个窗口开始。这种方法实现起来很简单，但可能会导致在新窗口开始时出现请求爆发。</li>
<li>滑动窗口速率限制:该算法改进了固定的窗口速率限制，允许在一个滑动窗口时间内(例如，在任何60秒间隔内)的最大请求数量。它减轻了固定窗口速率限制中出现的突发问题，并提供了对请求速率的更流畅的控制。</li>
<li>自适应速率限制:该算法根据当前系统负载、延迟或其他因素动态调整允许的请求速率。它的实现可能更加复杂，但通过实时适应不断变化的条件提供更好的资源管理。</li>
<li>指数回退:这是在重试机制中使用的策略，客户端逐渐增加重试之间的等待时间，以减少系统上的请求负载。每次请求失败后，客户端等待的时间呈指数增长，然后再发送另一个请求。这种方法在处理速率限制或瞬态错误时特别有用。</li>
</ol>
<h3 id="滑动窗口速率限制算法实现">滑动窗口速率限制算法实现</h3>
<div><pre class="hljs"><code><span class="hljs-keyword">package</span> main

<span class="hljs-keyword">import</span> (
    <span class="hljs-string">"fmt"</span>
    <span class="hljs-string">"sync"</span>
    <span class="hljs-string">"time"</span>
)

<span class="hljs-keyword">type</span> RateLimiter <span class="hljs-keyword">struct</span> {
    sync.Mutex
    maxRequests  <span class="hljs-keyword">int</span>
    windowLength time.Duration
    requests     <span class="hljs-keyword">map</span>[<span class="hljs-keyword">int64</span>]<span class="hljs-keyword">int</span>
}

<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">NewRateLimiter</span><span class="hljs-params">(maxRequests <span class="hljs-keyword">int</span>, windowLength time.Duration)</span> *<span class="hljs-title">RateLimiter</span></span> {
    <span class="hljs-keyword">return</span> &amp;RateLimiter{
        maxRequests:  maxRequests,
        windowLength: windowLength,
        requests:     <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-keyword">int64</span>]<span class="hljs-keyword">int</span>),
    }
}

<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *RateLimiter)</span> <span class="hljs-title">Allow</span><span class="hljs-params">()</span> <span class="hljs-title">bool</span></span> {
    r.Lock()
    <span class="hljs-keyword">defer</span> r.Unlock()

    now := time.Now().Unix()
    earliest := now - <span class="hljs-keyword">int64</span>(r.windowLength.Seconds())

    <span class="hljs-comment">// Clean up expired timestamps</span>
    <span class="hljs-keyword">for</span> timestamp := <span class="hljs-keyword">range</span> r.requests {
        <span class="hljs-keyword">if</span> timestamp &lt; earliest {
            <span class="hljs-built_in">delete</span>(r.requests, timestamp)
        }
    }

    <span class="hljs-comment">// Count requests in the current sliding window</span>
    requestCount := <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> _, count := <span class="hljs-keyword">range</span> r.requests {
        requestCount += count
    }

    <span class="hljs-comment">// Check if the request can be allowed</span>
    <span class="hljs-keyword">if</span> requestCount &lt; r.maxRequests {
        r.requests[now]++
        <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>
    }

    <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>
}

<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> {
    limiter := NewRateLimiter(<span class="hljs-number">5</span>, <span class="hljs-number">10</span>*time.Second)

    <span class="hljs-keyword">for</span> i := <span class="hljs-number">1</span>; i &lt;= <span class="hljs-number">10</span>; i++ {
        allowed := limiter.Allow()
        fmt.Printf(<span class="hljs-string">"Request %d: Allowed = %t\n"</span>, i, allowed)
        time.Sleep(<span class="hljs-number">2</span> * time.Second)
    }
}</code></pre></div>
<h3 id="参考资料">参考资料</h3>
<ul>
<li><a title="https://cloud.google.com/apis/docs/rate-limiting" href="https://cloud.google.com/apis/docs/rate-limiting">Rate limiting, API keys, and quotas (Google Cloud)</a></li>
<li><a title="https://www.toptal.com/nodejs/introduction-rate-limiting-nodejs" href="https://www.toptal.com/nodejs/introduction-rate-limiting-nodejs">Understanding and Implementing Rate Limiting in Node.js (Toptal)</a></li>
<li><a title="https://gist.github.com/peteti/6b89b1c6e91e6f9759b9a1a0e3a3df24" href="https://gist.github.com/peteti/6b89b1c6e91e6f9759b9a1a0e3a3df24">Sliding Window Rate Limiter in Golang (GitHub Gist)</a></li>
<li><a title="https://www.tomhudson.dev/blog/2020/07/building-a-rate-limiter-in-go" href="https://www.tomhudson.dev/blog/2020/07/building-a-rate-limiter-in-go">Building a rate limiter in Go (Tom Hudson's Blog)</a></li>
<li><a title="https://golangcode.com/rate-limiting-using-go/" href="https://golangcode.com/rate-limiting-using-go/">Rate limiting using the token bucket algorithm (Golang Code)</a></li>
<li><a title="https://redislabs.com/redis-best-practices/basic-rate-limiting/" href="https://redislabs.com/redis-best-practices/basic-rate-limiting/">Redis-based rate limiting (Redis Labs)</a></li>
<li><a title="https://www.nginx.com/blog/rate-limiting-nginx/" href="https://www.nginx.com/blog/rate-limiting-nginx/">Rate limiting with Nginx (Nginx)</a></li>
</ul>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[嵌入式时间序列数据库的具体时间]]></title>
            <guid>d13a4eb729314e1bb452bd2842e9f785</guid>
            <pubDate>Tue, 11 Apr 2023 13:54:46 GMT</pubDate>
            <content:encoded><![CDATA[<p>Here's an example of a simple embedded time series database using BoltDB as the storage engine in a Golang program:</p>
<ol>
<li>First, install the BoltDB package:</li>
</ol>
<div><pre class="hljs"><code>go get github.com/boltdb/bolt</code></pre></div>
<ol start="2">
<li>Create a new file, timeseries.go, and import the required packages:</li>
</ol>
<div><pre class="hljs"><code><span class="hljs-keyword">package</span> main

<span class="hljs-keyword">import</span> (
    <span class="hljs-string">"bytes"</span>
    <span class="hljs-string">"encoding/binary"</span>
    <span class="hljs-string">"encoding/gob"</span>
    <span class="hljs-string">"log"</span>
    <span class="hljs-string">"time"</span>

    <span class="hljs-string">"github.com/boltdb/bolt"</span>
)</code></pre></div>
<ol start="3">
<li>Define the data structures for the time series data point:</li>
</ol>
<div><pre class="hljs"><code><span class="hljs-keyword">type</span> DataPoint <span class="hljs-keyword">struct</span> {
    Timestamp time.Time
    Value     <span class="hljs-keyword">float64</span>
}</code></pre></div>
<ol start="4">
<li>Initialize the BoltDB database:</li>
</ol>
<div><pre class="hljs"><code><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">initDB</span><span class="hljs-params">()</span> *<span class="hljs-title">bolt</span>.<span class="hljs-title">DB</span></span> {
    db, err := bolt.Open(<span class="hljs-string">"timeseries.db"</span>, <span class="hljs-number">0600</span>, <span class="hljs-literal">nil</span>)
    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        log.Fatal(err)
    }
    <span class="hljs-keyword">return</span> db
}</code></pre></div>
<ol start="5">
<li>Create a bucket for storing time series data:</li>
</ol>
<div><pre class="hljs"><code><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">createBucket</span><span class="hljs-params">(db *bolt.DB, bucketName <span class="hljs-keyword">string</span>)</span></span> {
    err := db.Update(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(tx *bolt.Tx)</span> <span class="hljs-title">error</span></span> {
        _, err := tx.CreateBucketIfNotExists([]<span class="hljs-keyword">byte</span>(bucketName))
        <span class="hljs-keyword">return</span> err
    })

    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        log.Fatal(err)
    }
}</code></pre></div>
<ol start="6">
<li>Implement the <code>writeDataPoint</code> function:</li>
</ol>
<div><pre class="hljs"><code><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">writeDataPoint</span><span class="hljs-params">(db *bolt.DB, bucketName <span class="hljs-keyword">string</span>, dataPoint DataPoint)</span></span> {
    err := db.Update(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(tx *bolt.Tx)</span> <span class="hljs-title">error</span></span> {
        b := tx.Bucket([]<span class="hljs-keyword">byte</span>(bucketName))

        <span class="hljs-comment">// Serialize the DataPoint</span>
        buf := bytes.NewBuffer(<span class="hljs-literal">nil</span>)
        err := gob.NewEncoder(buf).Encode(dataPoint)
        <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
            <span class="hljs-keyword">return</span> err
        }

        <span class="hljs-comment">// Write the serialized DataPoint to the bucket</span>
        key := <span class="hljs-built_in">make</span>([]<span class="hljs-keyword">byte</span>, <span class="hljs-number">8</span>)
        binary.BigEndian.PutUint64(key, <span class="hljs-keyword">uint64</span>(dataPoint.Timestamp.UnixNano()))
        <span class="hljs-keyword">return</span> b.Put(key, buf.Bytes())
    })

    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        log.Fatal(err)
    }
}</code></pre></div>
<ol start="7">
<li>Implement the <code>readDataPoints</code> function:</li>
</ol>
<div><pre class="hljs"><code><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">readDataPoints</span><span class="hljs-params">(db *bolt.DB, bucketName <span class="hljs-keyword">string</span>)</span> []<span class="hljs-title">DataPoint</span></span> {
    <span class="hljs-keyword">var</span> dataPoints []DataPoint

    err := db.View(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(tx *bolt.Tx)</span> <span class="hljs-title">error</span></span> {
        b := tx.Bucket([]<span class="hljs-keyword">byte</span>(bucketName))

        c := b.Cursor()
        <span class="hljs-keyword">for</span> k, v := c.First(); k != <span class="hljs-literal">nil</span>; k, v = c.Next() {
            <span class="hljs-keyword">var</span> dataPoint DataPoint
            err := gob.NewDecoder(bytes.NewReader(v)).Decode(&amp;dataPoint)
            <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
                <span class="hljs-keyword">return</span> err
            }
            dataPoints = <span class="hljs-built_in">append</span>(dataPoints, dataPoint)
        }
        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>
    })

    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        log.Fatal(err)
    }

    <span class="hljs-keyword">return</span> dataPoints
}</code></pre></div>
<ol start="8">
<li>Use the database in the <code>main</code> function:</li>
</ol>
<div><pre class="hljs"><code><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> {
    db := initDB()
    <span class="hljs-keyword">defer</span> db.Close()

    bucketName := <span class="hljs-string">"timeseries"</span>

    createBucket(db, bucketName)

    dataPoint := DataPoint{
        Timestamp: time.Now(),
        Value:     <span class="hljs-number">42.0</span>,
    }
    writeDataPoint(db, bucketName, dataPoint)

    dataPoints := readDataPoints(db, bucketName)
    log.Println(dataPoints)
}</code></pre></div>
<p>This example demonstrates a very basic embedded time series database in Golang using BoltDB. You can extend this code to implement more advanced features like data partitioning, indexing, querying capabilities, and data management strategies as needed.</p>
<h2 id="索引的设计">索引的设计</h2>
<p>为时间序列数据库设计索引有助于提高数据检索效率和查询性能。在提供的使用BoltDB的示例中，您可以使用时间戳本身作为键，这已经提供了一种简单的索引形式。然而，如果你需要更高级的索引策略，你可以考虑以下方法:</p>
<ol>
<li>Time-based partitioning:<br />
根据时间戳将时间序列数据划分为更小的分区或bucket。每个分区可以表示一个固定的时间范围，例如一个小时、一天或一周。将每个分区单独存储在一个BoltDB桶中，可以提高数据的组织性和查询性能。</li>
</ol>
<div><pre class="hljs"><code><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">partitionKey</span><span class="hljs-params">(timestamp time.Time)</span> <span class="hljs-title">string</span></span> {
    <span class="hljs-comment">// Example: create partitions by day</span>
    <span class="hljs-keyword">return</span> timestamp.Format(<span class="hljs-string">"2006-01-02"</span>)
}</code></pre></div>
<ol start="2">
<li>Secondary indexing:<br />
如果时间序列数据具有附加属性，例如标记或标签，则可以创建辅助索引以加快基于这些属性的查询速度。要实现二级索引，您可以创建额外的BoltDB桶来存储对主数据点的引用。</li>
</ol>
<div><pre class="hljs"><code><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">writeSecondaryIndex</span><span class="hljs-params">(db *bolt.DB, indexBucketName <span class="hljs-keyword">string</span>, dataPointKey []<span class="hljs-keyword">byte</span>, indexValue <span class="hljs-keyword">string</span>)</span></span> {
    err := db.Update(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(tx *bolt.Tx)</span> <span class="hljs-title">error</span></span> {
        b := tx.Bucket([]<span class="hljs-keyword">byte</span>(indexBucketName))

        <span class="hljs-comment">// Store the data point key with the index value</span>
        <span class="hljs-keyword">return</span> b.Put([]<span class="hljs-keyword">byte</span>(indexValue), dataPointKey)
    })

    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        log.Fatal(err)
    }
}</code></pre></div>
<ol start="3">
<li>Time-based indexing:<br />
创建一个额外的索引，以加快需要特定时间范围或聚合的查询。例如，您可以创建一个索引来存储每个分区的最小值、最大值或平均值。这有助于加快需要在给定时间范围内计算汇总统计信息的查询。</li>
</ol>
<div><pre class="hljs"><code><span class="hljs-keyword">type</span> TimeRangeSummary <span class="hljs-keyword">struct</span> {
    Min <span class="hljs-keyword">float64</span>
    Max <span class="hljs-keyword">float64</span>
    Avg <span class="hljs-keyword">float64</span>
}

<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">writeTimeRangeIndex</span><span class="hljs-params">(db *bolt.DB, indexBucketName <span class="hljs-keyword">string</span>, partitionKey <span class="hljs-keyword">string</span>, summary TimeRangeSummary)</span></span> {
    err := db.Update(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(tx *bolt.Tx)</span> <span class="hljs-title">error</span></span> {
        b := tx.Bucket([]<span class="hljs-keyword">byte</span>(indexBucketName))

        <span class="hljs-comment">// Serialize the summary</span>
        buf := bytes.NewBuffer(<span class="hljs-literal">nil</span>)
        err := gob.NewEncoder(buf).Encode(summary)
        <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
            <span class="hljs-keyword">return</span> err
        }

        <span class="hljs-comment">// Write the serialized summary to the index bucket</span>
        <span class="hljs-keyword">return</span> b.Put([]<span class="hljs-keyword">byte</span>(partitionKey), buf.Bytes())
    })

    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        log.Fatal(err)
    }
}</code></pre></div>
<ol start="4">
<li>Updating and maintaining indexes:<br />
在添加新数据点或修改现有数据点时，请确保相应更新相应的索引。这可能需要更新二级索引或基于时间的索引结构，例如时间范围摘要统计信息。</li>
</ol>
<p>记住，建立索引是有代价的。虽然它可以显著提高查询性能，但也会增加数据库设计的复杂性，并增加写入操作的开销。必须分析应用程序的查询模式和需求，以确定最合适的索引策略。</p>
<h2 id="基于文件的删除策略">基于文件的删除策略</h2>
<p>To execute a delete strategy based on the database file size, you can monitor the file size periodically and trigger the deletion process once it exceeds a specified threshold. Here's how you can implement this strategy in your Golang program:</p>
<ol>
<li>Add a new global variable for the maximum allowed file size in bytes at the top of your <code>timeseries.go</code> file:</li>
</ol>
<div><pre class="hljs"><code><span class="hljs-keyword">const</span> maxFileSize <span class="hljs-keyword">int64</span> = <span class="hljs-number">1000000000</span> <span class="hljs-comment">// 1 GB</span></code></pre></div>
<ol start="2">
<li>Create a function to check the current database file size:</li>
</ol>
<div><pre class="hljs"><code><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">getFileSize</span><span class="hljs-params">(filepath <span class="hljs-keyword">string</span>)</span> <span class="hljs-params">(<span class="hljs-keyword">int64</span>, error)</span></span> {
    fileInfo, err := os.Stat(filepath)
    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>, err
    }
    <span class="hljs-keyword">return</span> fileInfo.Size(), <span class="hljs-literal">nil</span>
}</code></pre></div>
<ol start="3">
<li>Implement a function to delete a certain percentage of the oldest data points when the file size exceeds the threshold. The percentage is defined by the deletePercentage parameter:</li>
</ol>
<div><pre class="hljs"><code><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">deleteDataPointsByPercentage</span><span class="hljs-params">(db *bolt.DB, bucketName <span class="hljs-keyword">string</span>, deletePercentage <span class="hljs-keyword">float64</span>)</span></span> {
    <span class="hljs-keyword">if</span> deletePercentage &lt; <span class="hljs-number">0</span> || deletePercentage &gt; <span class="hljs-number">1</span> {
        log.Println(<span class="hljs-string">"Invalid delete percentage. It must be between 0 and 1."</span>)
        <span class="hljs-keyword">return</span>
    }

    err := db.Update(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(tx *bolt.Tx)</span> <span class="hljs-title">error</span></span> {
        b := tx.Bucket([]<span class="hljs-keyword">byte</span>(bucketName))
        c := b.Cursor()

        totalDataPoints := <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> k, _ := c.First(); k != <span class="hljs-literal">nil</span>; k, _ = c.Next() {
            totalDataPoints++
        }

        deleteCount := <span class="hljs-keyword">int</span>(<span class="hljs-keyword">float64</span>(totalDataPoints) * deletePercentage)

        <span class="hljs-keyword">for</span> i := <span class="hljs-number">0</span>; i &lt; deleteCount; i++ {
            k, _ := c.First()
            <span class="hljs-keyword">if</span> k != <span class="hljs-literal">nil</span> {
                <span class="hljs-keyword">if</span> err := c.Delete(); err != <span class="hljs-literal">nil</span> {
                    <span class="hljs-keyword">return</span> err
                }
            }
        }
        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>
    })

    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        log.Fatal(err)
    }
}</code></pre></div>
<ol start="4">
<li>Monitor the file size periodically in your <code>main</code> function and trigger the deletion when it exceeds the threshold. Use a ticker to check the file size at a specific interval, such as once per hour:</li>
</ol>
<div><pre class="hljs"><code><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> {
    db := initDB()
    <span class="hljs-keyword">defer</span> db.Close()

    bucketName := <span class="hljs-string">"timeseries"</span>

    createBucket(db, bucketName)

    <span class="hljs-comment">// Start the ticker for checking file size and triggering deletion</span>
    fileSizeTicker := time.NewTicker(<span class="hljs-number">1</span> * time.Hour)
    <span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> {
        <span class="hljs-keyword">for</span> <span class="hljs-keyword">range</span> fileSizeTicker.C {
            fileSize, err := getFileSize(<span class="hljs-string">"timeseries.db"</span>)
            <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
                log.Println(<span class="hljs-string">"Error checking file size:"</span>, err)
                <span class="hljs-keyword">continue</span>
            }

            <span class="hljs-keyword">if</span> fileSize &gt;= maxFileSize {
                log.Println(<span class="hljs-string">"Database file size exceeded threshold. Deleting oldest data points."</span>)
                deleteDataPointsByPercentage(db, bucketName, <span class="hljs-number">0.1</span>) <span class="hljs-comment">// Deletes 10% of the oldest data points</span>
            }
        }
    }()

    <span class="hljs-comment">// ... (the rest of your main function)</span>
}</code></pre></div>
<p>This implementation checks the database file size periodically and deletes a specified percentage of the oldest data points when the file size exceeds the threshold. Adjust the maxFileSize and deletePercentage variables according to your requirements. To minimize the deletion frequency, increase the file size threshold or decrease the percentage of data points to delete.</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[如何设计嵌入式时间序列数据库]]></title>
            <guid>e51232f5bfe94e348d8ed41115243834</guid>
            <pubDate>Tue, 11 Apr 2023 13:48:29 GMT</pubDate>
            <content:encoded><![CDATA[<p>在Golang程序中设计嵌入式时间序列数据库涉及一系列步骤，从选择合适的存储引擎到实现数据管理策略。下面是一个高级指南，帮助您在Golang中设计和实现嵌入式时间序列数据库:</p>
<ul>
<li>
<p>选择存储引擎: 对于嵌入式时间序列数据库，可以考虑使用嵌入式键值存储引擎，如BoltDB、Badger或LevelDB。这些引擎轻量级、快速且易于集成到Golang应用程序中。</p>
</li>
<li>
<p>定义数据模型: 设计一个有效地表示时间序列数据的数据模型。使用<code>structs</code>或<code>protobuf</code>这样的结构来定义数据模式。考虑使用时间戳作为键，使用序列化的数据点作为值，以实现高效的存储和检索。</p>
</li>
<li>
<p>设置存储引擎:</p>
<ul>
<li>将选择的存储引擎库导入到Golang程序中。</li>
<li>配置和初始化存储引擎，创建数据库实例。</li>
<li>实现打开、关闭和管理数据库连接的方法。</li>
</ul>
</li>
<li>
<p>实现数据存储和检索:</p>
<ul>
<li>创建从存储引擎写入和读取数据点的方法。</li>
<li>使用编码技术(如协议缓冲区、JSON或MessagePack)序列化和反序列化数据点。</li>
<li>使用批写和压缩优化存储和写性能。</li>
</ul>
</li>
<li>
<p>设计索引和分区:</p>
<ul>
<li>通过根据数据点的时间戳将数据点分组到桶中，实现基于时间的分区。</li>
<li>创建索引以实现高效的数据检索和查询。</li>
<li>利用存储引擎的内置索引功能或实现自定义索引解决方案。</li>
</ul>
</li>
<li>
<p>实现查询功能:</p>
<ul>
<li>设计一个简单的查询API或语言来与时间序列数据交互。</li>
<li>实现过滤、聚合和基于时间的操作的查询功能。</li>
<li>使用缓存、物化视图或预计算聚合优化查询性能。</li>
</ul>
</li>
<li>
<p>数据管理策略:</p>
<ul>
<li>实现数据保留策略，自动删除旧数据点或downsample数据，节省存储空间。</li>
<li>使用数据压缩技术可以减小存储空间大小并提高性能。</li>
</ul>
</li>
<li>
<p>测试和基准测试:</p>
<ul>
<li>为所有主要功能编写测试，以确保嵌入式时间序列数据库的正确性和可靠性。</li>
<li>对时间序列数据库的性能进行基准测试，以确定潜在的瓶颈和需要改进的领域。</li>
</ul>
</li>
</ul>
<p>通过执行这些步骤，您可以在Golang程序中设计和实现高效的嵌入式时间序列数据库。确保持续监控、优化和维护数据库，以确保其可靠性和性能</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[如何大规模配置裸机电信云平台]]></title>
            <guid>df4af15487b943e5938ac6f9cfc97490</guid>
            <pubDate>Wed, 01 Feb 2023 16:39:35 GMT</pubDate>
            <content:encoded><![CDATA[<blockquote>
<p>使用通过 OpenShift 实施的 Kubernetes 大规模通过和配置裸机集群。</p>
</blockquote>
<p>自动化系统供应对于 IT 或网络系统管理员来说至关重要。许多技术和标准试图解决这个问题；有些是供应商中立的，有些是特定于供应商的。后者在无线网络中尤其如此，其中每个网络设备提供商都需要其关联的元素/网络管理系统来控制其网络元素。</p>
<p>鉴于无线电接入网络 (RAN) 分解和解聚的兴起，Open RAN 等标准机构专注于定义 RAN 组件无线电单元 (RU)、分布式单元 (DU) 和中央单元 (CU) 之间的可互操作接口。与此同时，在 O-RAN 联盟的领导下，行业推动 RAN 组件虚拟化和云化，因此它们的生命周期可以独立于运行它们的平台进行管理。这种由商用现成 (COTS) 服务器组成的平台需要高度配置才能发挥 5G 的优势。</p>
<p><img src="/_resources/480c1db4ac474a9fad80ca3db516e577.png" /></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline" class="jop-noMdConv">Alexis de Talhouët &amp; Federico Rossi, CC BY-SA 4.0</center>
<p>这带来的挑战之一是大规模供应和配置该平台的能力；一个电信服务提供商可能有数千个 RAN 部署。该平台包括服务器和管理程序或容器编排系统。</p>
<p>本文将使用OpenShift实现的容器编排系统Kubernetes；特别是 Red Hat OpenShift 4.11。</p>
<h2 id="解决方案概述">解决方案概述</h2>
<p>与 Red Hat 和合作伙伴一起部署云原生 5G 开放式 RAN 提出了要求和高级方法。它包含三个主要元素：</p>
<ul>
<li>用于大规模部署的云原生平台</li>
<li>GitOps 的自动化和操作</li>
<li>蓬勃发展的合作伙伴生态系统</li>
</ul>
<p>该解决方案旨在通过利用 Red Hat OpenShift 提供的一组广为采用的通用 API（无论是在中央、区域还是边缘区域）在整个环境中提供相同的管理、操作和用户体验。该水平平台不一定是一个实例，而是一组共同管理的实例。</p>
<p>这些 RAN 部署模型使用一个管理环境来控制其他模型，将它们视为牛而不是宠物。它被称为管理集群或中心集群。远程环境称为托管集群。</p>
<p>下图描述了由 Red Hat Advanced Cluster Management (RHACM) 启用的中央实体如何配置和操作由以下任何一项组成的远程环境：</p>
<ul>
<li>一个完整的 OpenShift 集群（三个控制平面和至少两个计算节点）</li>
<li>一个紧凑的 OpenShift 集群（三个节点同时充当控制和计算）</li>
<li>由一个节点组成的单节点OpenShift</li>
<li>远程工作节点（远程连接到控制平面，托管在管理集群中，或远程）</li>
</ul>
<p><img src="/_resources/95b6c8a6deb2409da084e89c743082a2.png" /></p>
<p>最后，要定义一次，随处部署，并持续协调运行状态与真实来源，请遵循 GitOps 方法，例如 Red Hat OpenShift GitOps。</p>
<p>大规模采用 GitOps 存在不同的实施模型，但这些不是本文的主题。</p>
<p><img src="/_resources/84aa23769b3640ce89928c8fb45f5958.png" /></p>
<h2 id="远程裸机集群生命周期管理">远程裸机集群生命周期管理</h2>
<p>OpenStack 社区首先通过 Ironic 项目解决了这一需求。目标是通过通用（预引导执行环境 [PXE] 和智能平台管理接口 [IPMI]）和特定于供应商的远程管理协议（底板管理控制器 [BMC]）来管理硬件。该项目成为 <a title="https://metal3.io/" href="https://metal3.io/">Metal3</a> 的垫脚石，为 Kubernetes 启用了此类功能。</p>
<p>使用<a title="https://console.redhat.com/openshift/assisted-installer/clusters" href="https://console.redhat.com/openshift/assisted-installer/clusters">红帽 SaaS 辅助安装程序平台</a>在红帽混合云控制台上部署第一个集群非常简单，即使您使用自己的映像注册表也是如此。阅读<a title="https://cloud.redhat.com/blog/making-openshift-on-bare-metal-easy" href="https://cloud.redhat.com/blog/making-openshift-on-bare-metal-easy">使裸机上的 OpenShift 变得简单</a>，以全面了解该过程。</p>
<p>主要要求是出站互联网访问；不需要入站连接。辅助安装程序是一项服务：</p>
<ul>
<li>允许基础架构管理员生成发现图像</li>
<li>发现基础架构管理员使用发现映像启动的主机</li>
<li>允许集群创建者从可用主机定义集群</li>
<li>在开始之前验证安装</li>
<li>监控装置</li>
<li>收集日志</li>
<li>发布指标供内部使用</li>
<li>允许集群创建者将主机添加到现有集群</li>
</ul>
<p>创建第一个集群后，您必须安装 RHACM 和 Red Hat OpenShift GitOps。接下来，您可以按照 GitOps 方法论来创建和配置后续集群。</p>
<p><a title="https://cloud.redhat.com/blog/zero-touch-provisioning-for-factory-workflows" href="https://cloud.redhat.com/blog/zero-touch-provisioning-for-factory-workflows">工厂工作流程的零接触配置</a>是了解启用裸机服务器远程配置以实现全自动集群安装的策略的好方法。它在引擎盖下使用 Metal3 并严重依赖 BMC 和配置接口 MAC 地址。</p>
<p>在高层次上，您将使用工厂集群部署一个独立的集群，如下所示：</p>
<ol>
<li>安装工厂集群：创建工厂集群（管理）创建OpenShift集群。</li>
<li>创建边缘集群：使用工厂集群在 OEM 硬件上创建完全可操作的 OpenShift 集群（辐射）。</li>
<li>配置边缘集群：在最终客户站点（边缘）解压并配置 OpenShift 集群。</li>
</ol>
<p>此外，RHACM 还提供站点规划和策略模板，支持大规模部署和配置集群。</p>
<p>在<a title="https://docs.openshift.com/container-platform/4.11/scalability_and_performance/ztp-deploying-disconnected.html" href="https://docs.openshift.com/container-platform/4.11/scalability_and_performance/ztp-deploying-disconnected.html">文档</a>中查找更多详细信息。</p>
<p><img src="/_resources/8c611371be344eb88c49de5a88b8fa5d.png" /></p>
<p>##. 拓扑感知生命周期管理</p>
<p>需要一个强大的基础设施生命周期管理工具来满足在整个生命周期内大规模管理基础设施的技术和业务需求。</p>
<p>它应该考虑以下几点：</p>
<ol>
<li>定义最低可用性要求的服务级别协议 (SLA)</li>
<li>需要逐步向队列推出的操作程序（例如，作为初始冒烟测试升级的金丝雀集群）</li>
<li>大规模自动化要求</li>
</ol>
<p>拓扑感知生命周期操作员 （TALO） 在操作环境中大规模管理这些事件。</p>
<p><a title="https://www.redhat.com/en/engage/discover-automating-edge-20220510?intcmp=7013a0000025wJwAAI" href="https://www.redhat.com/en/engage/discover-automating-edge-20220510?intcmp=7013a0000025wJwAAI">Automating the last mile: Ensuring consistency and scalability at the edge</a></p>
<p>一个示例用例是具有 RAN 覆盖范围的大型集群 （DU） 队列。更新分波次应用，以确保保持足够的 RAN 覆盖范围。SLA 指定覆盖区域内维护事件期间不可用群集的最大数量。</p>
<p><img src="/_resources/c3702dd1798242aebddf8376878c10df.png" /></p>
<ul>
<li>绿色和蓝色集群（集群组升级 [CGU] 1 和 CGU 2）配对以提供服务覆盖范围重叠。</li>
<li>操作程序规定可以更新三个集群（最多）。</li>
<li>CGU 1 以 3 个为批次更新绿色簇。</li>
<li>CGU 2 与 CGU 1 链接，并在 CGU 1 完成时以三个为一组更新蓝色集群。</li>
</ul>
<p>TALO 通过提供集成的备份和恢复（回滚）功能，将维护窗口期间的风险降至最低。此功能可创建升级前备份，并提供在升级失败时快速恢复单节点 OpenShift （SNO） 的过程。如果升级失败，此功能允许将 SNO 恢复到以前版本的 OpenShift 的工作状态，而无需重新配置应用程序。</p>
<p>群集升级需要在维护时段内完成。该窗口还需要包括解决失败升级的时间。必须将 OpenShift 版本工件下载到集群才能进行升级。某些群集托管注册表的中心群集的带宽可能有限，因此很难在所需时间内完成升级。在升级之前，群集上必须存在所需的项目，以确保升级适合维护时段。</p>
<p>为了实现这些操作过程，TALO 以声明性方式管理一个或多个 OpenShift 集群的 RHACM 策略部署。</p>
<h2 id="云原生平台配置">云原生平台配置</h2>
<p>该方法可以大规模部署和管理群集。查看每个群集的较低级别组件以启用性能调整。这些是构成上述政策的要素。</p>
<p>目标是使用 Kubernetes 资源模型完成 100% 的硬件配置，支持将预期配置声明为代码，并通过 GitOps 促进操作管理。</p>
<p>下图显示了节点操作系统和所有支持的运算符，启用了特定的节点配置。</p>
<p><img src="/_resources/fa48f03b23a94197ac92586764c46514.png" /></p>
<p>以下是5G核心网应用最重要的运营商：</p>
<ul>
<li><a title="https://docs.openshift.com/container-platform/4.11/nodes/nodes/nodes-node-tuning-operator.html" href="https://docs.openshift.com/container-platform/4.11/nodes/nodes/nodes-node-tuning-operator.html">Node Tuning Operator</a>可用于优化基础操作系统以减少延迟。它使用性能配置文件，并负责：
<ul>
<li>HugePages</li>
<li>Topology manager policy (NUMA)</li>
<li>CPU reservation and isolation</li>
<li>Power consumption scheme</li>
<li>Real-time kernel</li>
<li>Network interface controller (NIC) queues tuning (DPDK)</li>
<li>IRQ dynamic load-balancing</li>
</ul>
</li>
<li><a title="https://docs.openshift.com/container-platform/4.11/hardware_enablement/psap-special-resource-operator.html" href="https://docs.openshift.com/container-platform/4.11/hardware_enablement/psap-special-resource-operator.html">Special Resource Operator</a> 允许您加载特定的内核模块和设备驱动程序。此组件仍处于技术预览阶段，这意味着不应在生产中使用它。如果要使用它，建议获取指导。</li>
<li><a title="https://docs.openshift.com/container-platform/4.11/networking/k8s_nmstate/k8s-nmstate-about-the-k8s-nmstate-operator.html" href="https://docs.openshift.com/container-platform/4.11/networking/k8s_nmstate/k8s-nmstate-about-the-k8s-nmstate-operator.html">NMState Operator</a> 启用接口配置，无论其类型如何。它通常用于定义绑定接口以及 VLAN 子接口。</li>
<li><a title="https://docs.openshift.com/container-platform/4.11/networking/hardware_networks/about-sriov.html" href="https://docs.openshift.com/container-platform/4.11/networking/hardware_networks/about-sriov.html">SR-IOV Operator</a>允许通过 SriovNetworkNodePolicy 配置 SR-IOV 支持的网卡，该策略配置：
<ul>
<li>MTU</li>
<li>Number of VFs</li>
<li>NIC selector</li>
<li>Device type (whether vfio-pci or netdev)<br />
需要 SriovNetwork 才能使用创建的设备。这将产生一个 NetworkAttachmentDefinition，然后 Pod 可以通过注释使用该定义。</li>
</ul>
</li>
</ul>
<p>5G RAN 或其他类型的工作负载还有其他几个运营商，例如Precision Time Protocol Operator, Intel Device Plugin Operator, FEC Operator, NVIDIA GPU Operator, and Network Operator.。每个都旨在抽象特定于硬件的功能，并使用 Kubernetes 资源模型启用其配置。</p>
<p>注意：许多operator支持告警、监控、服务保障、远程管理、证书管理等。</p>
<p>按应用程序和基础架构类型定义性能配置文件是释放性能的关键。</p>
<p>下图是 DU 性能配置文件所需的运算符示例。</p>
<p><img src="/_resources/cb79ca5609c94a50b6850813dca7dd0b.png" /></p>
<h2 id="展望未来">展望未来</h2>
<p>大规模预置和配置裸机集群的能力现已成为现实。但其他挑战依然存在，包括：</p>
<ul>
<li>使用统一的管理平台观察所有这些环境进行操作</li>
<li>实现自主系统（平台和网络），促进服务水平保证的可持续性</li>
<li>用于无缝应用通信的多集群网络（跨中央和边缘集群的分布式 SBA 5G 核心网）</li>
<li>增强调度能力，实现可持续计算</li>
</ul>
<p>请继续关注更多信息。同时，请随时查看我们在 <a title="https://www.redhat.com/en/events/red-hat-kubecon-cloudnativecon-na-2022" href="https://www.redhat.com/en/events/red-hat-kubecon-cloudnativecon-na-2022">KubeCon NA 2022</a> 上完成和展示的一些进展。</p>
<p><a title="https://www.redhat.com/architect/cloud-provisioning-scale" href="https://www.redhat.com/architect/cloud-provisioning-scale">原文连接</a></p>
]]></content:encoded>
        </item>
    </channel>
</rss>